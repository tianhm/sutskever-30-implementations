{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 29: Retrieval-Augmented Generation for Knowledge-Intensive Tasks\n",
    "## Patrick Lewis, Ethan Perez, Aleksandra Piktus, et al., Meta AI (2020)\n",
    "\n",
    "### RAG: Retrieval-Augmented Generation\n",
    "\n",
    "Combine dense retrieval (DPR) with seq2seq generation (BART). Best of both worlds: external knowledge + powerful generation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Architecture\n",
    "\n",
    "```\n",
    "Input query (x)\n",
    "    ↓\n",
    "Retriever (DPR) → Top-k documents (z)\n",
    "    ↓\n",
    "Generator (BART) → P(y | x, z)\n",
    "    ↓\n",
    "Output (y)\n",
    "```\n",
    "\n",
    "**Two variants:**\n",
    "- **RAG-Sequence**: Marginalize over documents for entire sequence\n",
    "- **RAG-Token**: Marginalize over documents per token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "class SimpleRetriever:\n",
    "    \"\"\"Simplified dense retriever (like DPR)\"\"\"\n",
    "    def __init__(self, embedding_dim):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.query_encoder_W = np.random.randn(embedding_dim, embedding_dim) * 0.01\n",
    "    \n",
    "    def encode_query(self, query_tokens):\n",
    "        \"\"\"Encode query to dense vector\"\"\"\n",
    "        # Simplified: just use random projection\n",
    "        query_vec = np.mean(query_tokens, axis=0)\n",
    "        encoded = np.dot(self.query_encoder_W, query_vec)\n",
    "        # L2 normalize\n",
    "        return encoded / (np.linalg.norm(encoded) + 1e-8)\n",
    "    \n",
    "    def retrieve(self, query_embedding, document_embeddings, k=5):\n",
    "        \"\"\"\n",
    "        Retrieve top-k documents\n",
    "        Returns: indices and probabilities\n",
    "        \"\"\"\n",
    "        # Compute similarities\n",
    "        similarities = np.dot(document_embeddings, query_embedding)\n",
    "        \n",
    "        # Get top-k\n",
    "        top_k_indices = np.argsort(similarities)[::-1][:k]\n",
    "        top_k_scores = similarities[top_k_indices]\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        probs = softmax(top_k_scores)\n",
    "        \n",
    "        return top_k_indices, probs\n",
    "\n",
    "# Test retriever\n",
    "embedding_dim = 64\n",
    "retriever = SimpleRetriever(embedding_dim)\n",
    "\n",
    "# Dummy data\n",
    "query_tokens = np.random.randn(10, embedding_dim)\n",
    "document_embeddings = np.random.randn(20, embedding_dim)\n",
    "# Normalize documents\n",
    "document_embeddings = document_embeddings / (np.linalg.norm(document_embeddings, axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "query_emb = retriever.encode_query(query_tokens)\n",
    "top_indices, top_probs = retriever.retrieve(query_emb, document_embeddings, k=5)\n",
    "\n",
    "print(f\"Retrieved documents: {top_indices}\")\n",
    "print(f\"Retrieval probabilities: {top_probs}\")\n",
    "print(f\"Sum of probs: {np.sum(top_probs):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator (Seq2Seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGenerator:\n",
    "    \"\"\"Simplified seq2seq generator (like BART)\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder_W = np.random.randn(hidden_dim, embedding_dim) * 0.01\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder_W = np.random.randn(hidden_dim, embedding_dim) * 0.01\n",
    "        self.output_W = np.random.randn(vocab_size, hidden_dim) * 0.01\n",
    "    \n",
    "    def generate_prob(self, query_tokens, doc_tokens, target_tokens):\n",
    "        \"\"\"\n",
    "        Compute P(y | x, z) where:\n",
    "        - x: query\n",
    "        - z: document\n",
    "        - y: target output\n",
    "        \"\"\"\n",
    "        # Encode query + document\n",
    "        combined = np.concatenate([query_tokens, doc_tokens], axis=0)\n",
    "        encoder_hidden = np.tanh(np.dot(self.encoder_W, np.mean(combined, axis=0)))\n",
    "        \n",
    "        # Decode target\n",
    "        log_prob = 0\n",
    "        for target_token in target_tokens:\n",
    "            decoder_hidden = np.tanh(np.dot(self.decoder_W, target_token))\n",
    "            \n",
    "            # Combine encoder and decoder\n",
    "            combined_hidden = encoder_hidden + decoder_hidden\n",
    "            \n",
    "            # Output distribution\n",
    "            logits = np.dot(self.output_W, combined_hidden)\n",
    "            probs = softmax(logits)\n",
    "            \n",
    "            # Assume we know the target token index (simplified)\n",
    "            # In reality, we'd compute cross-entropy\n",
    "            target_idx = np.argmax(target_token)  # One-hot\n",
    "            log_prob += np.log(probs[target_idx] + 1e-8)\n",
    "        \n",
    "        return log_prob\n",
    "\n",
    "# Test generator\n",
    "vocab_size = 1000\n",
    "generator = SimpleGenerator(vocab_size, embedding_dim, hidden_dim=128)\n",
    "\n",
    "# Dummy tokens (embeddings)\n",
    "query = np.random.randn(5, embedding_dim)\n",
    "doc = np.random.randn(20, embedding_dim)\n",
    "target = np.random.randn(8, embedding_dim)\n",
    "\n",
    "log_prob = generator.generate_prob(query, doc, target)\n",
    "print(f\"\\nLog P(y | x, z): {log_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG-Sequence: Marginalize Over Documents\n",
    "\n",
    "$$\n",
    "P_{RAG-Seq}(y | x) = \\sum_{z \\in \\text{top-k}} P(z | x) \\cdot P(y | x, z)\n",
    "$$\n",
    "\n",
    "Generate entire sequence with each document, then combine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGSequence:\n",
    "    \"\"\"RAG-Sequence model\"\"\"\n",
    "    def __init__(self, retriever, generator):\n",
    "        self.retriever = retriever\n",
    "        self.generator = generator\n",
    "    \n",
    "    def forward(self, query_tokens, target_tokens, document_embeddings, documents_tokens, k=5):\n",
    "        \"\"\"\n",
    "        RAG-Sequence forward pass\n",
    "        \n",
    "        P(y|x) = Σ_z P(z|x) * P(y|x,z)\n",
    "        \"\"\"\n",
    "        # Retrieve documents\n",
    "        query_emb = self.retriever.encode_query(query_tokens)\n",
    "        doc_indices, doc_probs = self.retriever.retrieve(query_emb, document_embeddings, k=k)\n",
    "        \n",
    "        # Marginalize over documents\n",
    "        total_prob = 0\n",
    "        \n",
    "        for doc_idx, p_z_given_x in zip(doc_indices, doc_probs):\n",
    "            # Get document tokens\n",
    "            doc_tokens = documents_tokens[doc_idx]\n",
    "            \n",
    "            # P(y | x, z)\n",
    "            log_p_y_given_xz = self.generator.generate_prob(query_tokens, doc_tokens, target_tokens)\n",
    "            p_y_given_xz = np.exp(log_p_y_given_xz)\n",
    "            \n",
    "            # P(z|x) * P(y|x,z)\n",
    "            total_prob += p_z_given_x * p_y_given_xz\n",
    "        \n",
    "        return np.log(total_prob + 1e-8), doc_indices, doc_probs\n",
    "\n",
    "# Create RAG-Sequence model\n",
    "rag_seq = RAGSequence(retriever, generator)\n",
    "\n",
    "# Generate dummy documents\n",
    "num_docs = 20\n",
    "documents_tokens = [np.random.randn(15, embedding_dim) for _ in range(num_docs)]\n",
    "\n",
    "# Test\n",
    "log_prob, used_docs, used_probs = rag_seq.forward(\n",
    "    query_tokens=query,\n",
    "    target_tokens=target,\n",
    "    document_embeddings=document_embeddings,\n",
    "    documents_tokens=documents_tokens,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(\"\\nRAG-Sequence:\")\n",
    "print(f\"Log P(y|x): {log_prob:.4f}\")\n",
    "print(f\"Used documents: {used_docs}\")\n",
    "print(f\"Document weights: {used_probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG-Token: Marginalize Per Token\n",
    "\n",
    "$$\n",
    "P_{RAG-Token}(y | x) = \\prod_{i=1}^{|y|} \\sum_{z \\in \\text{top-k}} P(z | x) \\cdot P(y_i | x, z, y_{<i})\n",
    "$$\n",
    "\n",
    "Can use different documents for different tokens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGToken:\n",
    "    \"\"\"RAG-Token model (simplified)\"\"\"\n",
    "    def __init__(self, retriever, generator):\n",
    "        self.retriever = retriever\n",
    "        self.generator = generator\n",
    "    \n",
    "    def forward_token(self, query_tokens, target_token, document_embeddings, documents_tokens, k=5):\n",
    "        \"\"\"\n",
    "        Compute P(y_i | x) for single token\n",
    "        \n",
    "        P(y_i | x) = Σ_z P(z|x) * P(y_i|x,z)\n",
    "        \"\"\"\n",
    "        # Retrieve documents\n",
    "        query_emb = self.retriever.encode_query(query_tokens)\n",
    "        doc_indices, doc_probs = self.retriever.retrieve(query_emb, document_embeddings, k=k)\n",
    "        \n",
    "        # Marginalize for this token\n",
    "        token_prob = 0\n",
    "        \n",
    "        for doc_idx, p_z_given_x in zip(doc_indices, doc_probs):\n",
    "            doc_tokens = documents_tokens[doc_idx]\n",
    "            \n",
    "            # P(y_i | x, z) - simplified\n",
    "            log_p = self.generator.generate_prob(query_tokens, doc_tokens, [target_token])\n",
    "            p_yi_given_xz = np.exp(log_p)\n",
    "            \n",
    "            token_prob += p_z_given_x * p_yi_given_xz\n",
    "        \n",
    "        return token_prob, doc_indices, doc_probs\n",
    "    \n",
    "    def forward(self, query_tokens, target_tokens, document_embeddings, documents_tokens, k=5):\n",
    "        \"\"\"\n",
    "        Full sequence probability\n",
    "        \n",
    "        P(y|x) = ∏_i P(y_i|x)\n",
    "        \"\"\"\n",
    "        log_prob_total = 0\n",
    "        \n",
    "        for target_token in target_tokens:\n",
    "            token_prob, _, _ = self.forward_token(\n",
    "                query_tokens, target_token, document_embeddings, documents_tokens, k\n",
    "            )\n",
    "            log_prob_total += np.log(token_prob + 1e-8)\n",
    "        \n",
    "        return log_prob_total\n",
    "\n",
    "# Create RAG-Token model\n",
    "rag_token = RAGToken(retriever, generator)\n",
    "\n",
    "# Test\n",
    "log_prob_token = rag_token.forward(\n",
    "    query_tokens=query,\n",
    "    target_tokens=target,\n",
    "    document_embeddings=document_embeddings,\n",
    "    documents_tokens=documents_tokens,\n",
    "    k=5\n",
    ")\n",
    "\n",
    "print(\"\\nRAG-Token:\")\n",
    "print(f\"Log P(y|x): {log_prob_token:.4f}\")\n",
    "print(\"\\nDifference: RAG-Token can use different docs per token!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic QA Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create more realistic example\n",
    "knowledge_base = [\n",
    "    \"The Eiffel Tower was built in 1889 by Gustave Eiffel.\",\n",
    "    \"Paris is the capital of France and has a population of 2.2 million.\",\n",
    "    \"The Statue of Liberty was a gift from France to the United States.\",\n",
    "    \"Mount Everest is 8,849 meters tall and located in the Himalayas.\",\n",
    "    \"The Amazon River flows through South America for 6,400 kilometers.\",\n",
    "]\n",
    "\n",
    "qa_pairs = [\n",
    "    (\"When was the Eiffel Tower built?\", \"1889\", 0),\n",
    "    (\"What is the height of Mount Everest?\", \"8,849 meters\", 3),\n",
    "    (\"How long is the Amazon River?\", \"6,400 kilometers\", 4),\n",
    "]\n",
    "\n",
    "print(\"Knowledge Base:\")\n",
    "for i, doc in enumerate(knowledge_base):\n",
    "    print(f\"  {i}. {doc}\")\n",
    "\n",
    "print(\"\\nQA Pairs:\")\n",
    "for q, a, doc_idx in qa_pairs:\n",
    "    print(f\"  Q: {q}\")\n",
    "    print(f\"  A: {a}\")\n",
    "    print(f\"  Relevant doc: #{doc_idx}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize RAG Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "def draw_rag_variant(ax, title, is_token=False):\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 12)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Query\n",
    "    ax.add_patch(plt.Rectangle((4, 10.5), 2, 0.8, fill=True, \n",
    "                               color='lightblue', ec='black', linewidth=2))\n",
    "    ax.text(5, 10.9, 'Query (x)', ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    # Retriever\n",
    "    ax.add_patch(plt.Rectangle((3.5, 9), 3, 1, fill=True, \n",
    "                               color='lightgreen', ec='black', linewidth=2))\n",
    "    ax.text(5, 9.5, 'Retriever\\n(DPR)', ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    ax.arrow(5, 10.5, 0, -0.3, head_width=0.2, head_length=0.1, fc='black', ec='black', linewidth=2)\n",
    "    \n",
    "    # Retrieved documents\n",
    "    doc_positions = [2, 4, 6, 8]\n",
    "    ax.text(5, 7.8, 'Top-k Documents', ha='center', fontsize=10, fontweight='bold')\n",
    "    for i, x in enumerate(doc_positions[:3]):\n",
    "        ax.add_patch(plt.Rectangle((x-0.4, 6.5), 0.8, 1, fill=True, \n",
    "                                   color='lightyellow', ec='black', linewidth=1.5))\n",
    "        ax.text(x, 7, f'z{i+1}', ha='center', va='center', fontsize=9)\n",
    "        # Arrow from retriever\n",
    "        ax.plot([5, x], [9, 7.5], 'k--', alpha=0.5, linewidth=1)\n",
    "    \n",
    "    if not is_token:\n",
    "        # RAG-Sequence: each doc generates full sequence\n",
    "        y_positions = [2, 4, 6]\n",
    "        for i, (dx, dy) in enumerate(zip(doc_positions[:3], y_positions)):\n",
    "            # Generator per document\n",
    "            ax.add_patch(plt.Rectangle((dy-0.5, 4.5), 1, 0.8, fill=True, \n",
    "                                       color='lightcoral', ec='black', linewidth=1.5))\n",
    "            ax.text(dy, 4.9, f'Gen', ha='center', va='center', fontsize=8)\n",
    "            ax.arrow(dx, 6.5, dy-dx, -1.5, head_width=0.15, head_length=0.1, \n",
    "                    fc='gray', ec='gray', linewidth=1, alpha=0.6)\n",
    "            \n",
    "            # Output sequence\n",
    "            ax.add_patch(plt.Rectangle((dy-0.6, 3), 1.2, 0.6, fill=True, \n",
    "                                       color='wheat', ec='black', linewidth=1))\n",
    "            ax.text(dy, 3.3, f'y', ha='center', va='center', fontsize=8)\n",
    "            ax.arrow(dy, 4.5, 0, -0.8, head_width=0.12, head_length=0.08, \n",
    "                    fc='black', ec='black', linewidth=1)\n",
    "        \n",
    "        # Combine\n",
    "        ax.add_patch(plt.Rectangle((4, 1.2), 2, 0.8, fill=True, \n",
    "                                   color='plum', ec='black', linewidth=2))\n",
    "        ax.text(5, 1.6, 'Σ P(z|x)P(y|x,z)', ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "        for dy in y_positions:\n",
    "            ax.plot([dy, 5], [3, 2], 'k-', alpha=0.5, linewidth=1.5)\n",
    "    else:\n",
    "        # RAG-Token: combine docs for each token\n",
    "        token_y = 4.5\n",
    "        for t in range(3):\n",
    "            tx = 2 + t * 2.5\n",
    "            \n",
    "            # Token position\n",
    "            ax.add_patch(plt.Rectangle((tx-0.4, token_y), 0.8, 0.6, fill=True, \n",
    "                                       color='lightcoral', ec='black', linewidth=1.5))\n",
    "            ax.text(tx, token_y+0.3, f'y{t+1}', ha='center', va='center', fontsize=9)\n",
    "            \n",
    "            # Arrows from all docs\n",
    "            for dx in doc_positions[:3]:\n",
    "                ax.plot([dx, tx], [6.5, token_y+0.6], 'k--', alpha=0.3, linewidth=0.8)\n",
    "        \n",
    "        # Final output\n",
    "        ax.add_patch(plt.Rectangle((3.5, 2.5), 3, 0.8, fill=True, \n",
    "                                   color='plum', ec='black', linewidth=2))\n",
    "        ax.text(5, 2.9, '∏ Σ P(z|x)P(yi|x,z)', ha='center', va='center', \n",
    "               fontsize=9, fontweight='bold')\n",
    "        ax.arrow(4, token_y, 0.8, -1.3, head_width=0.15, head_length=0.1, \n",
    "                fc='black', ec='black', linewidth=1.5, alpha=0.5)\n",
    "    \n",
    "    # Final answer\n",
    "    ax.add_patch(plt.Rectangle((4, 0.3), 2, 0.6, fill=True, \n",
    "                               color='lightgreen', ec='black', linewidth=2))\n",
    "    ax.text(5, 0.6, 'Answer', ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "    ax.arrow(5, 1.2 if not is_token else 2.5, 0, \n",
    "            -0.2 if not is_token else -1.5, \n",
    "            head_width=0.2, head_length=0.1, fc='green', ec='green', linewidth=2)\n",
    "\n",
    "draw_rag_variant(axes[0], 'RAG-Sequence', is_token=False)\n",
    "draw_rag_variant(axes[1], 'RAG-Token', is_token=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare RAG Variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate probabilities for visualization\n",
    "n_docs = 5\n",
    "n_tokens = 8\n",
    "\n",
    "# RAG-Sequence: same doc weights for all tokens\n",
    "doc_weights_seq = softmax(np.random.randn(n_docs))\n",
    "weights_seq_matrix = np.tile(doc_weights_seq, (n_tokens, 1))\n",
    "\n",
    "# RAG-Token: different doc weights per token\n",
    "weights_token_matrix = np.array([softmax(np.random.randn(n_docs)) for _ in range(n_tokens)])\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "im1 = ax1.imshow(weights_seq_matrix.T, cmap='YlOrRd', aspect='auto', vmin=0, vmax=1)\n",
    "ax1.set_xlabel('Output Token Position', fontsize=12)\n",
    "ax1.set_ylabel('Document', fontsize=12)\n",
    "ax1.set_title('RAG-Sequence\\n(Same docs for all tokens)', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(im1, ax=ax1, label='P(z|x)')\n",
    "\n",
    "im2 = ax2.imshow(weights_token_matrix.T, cmap='YlOrRd', aspect='auto', vmin=0, vmax=1)\n",
    "ax2.set_xlabel('Output Token Position', fontsize=12)\n",
    "ax2.set_ylabel('Document', fontsize=12)\n",
    "ax2.set_title('RAG-Token\\n(Different docs per token)', fontsize=13, fontweight='bold')\n",
    "plt.colorbar(im2, ax=ax2, label='P(z|x)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRAG-Sequence: More consistent (uses same knowledge)\")\n",
    "print(\"RAG-Token: More flexible (can mix knowledge sources)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### RAG Architecture:\n",
    "\n",
    "**Components**:\n",
    "1. **Retriever**: Dense retrieval (DPR-style)\n",
    "   - Query encoder: $q_{emb} = E_Q(x)$\n",
    "   - Document encoder: $d_{emb} = E_D(z)$\n",
    "   - Retrieval: $P(z|x) \\propto \\exp(q_{emb} \\cdot d_{emb})$\n",
    "\n",
    "2. **Generator**: Seq2seq model (BART)\n",
    "   - Input: query $x$ + document $z$\n",
    "   - Output: $P(y | x, z)$\n",
    "\n",
    "### RAG-Sequence:\n",
    "\n",
    "$$\n",
    "P_{RAG-Seq}(y | x) = \\sum_{z \\in \\text{top-k}} P(z | x) \\cdot P_{seq2seq}(y | x, z)\n",
    "$$\n",
    "\n",
    "**Process**:\n",
    "1. Retrieve top-k documents\n",
    "2. Generate full sequence with each document\n",
    "3. Weighted sum of sequences\n",
    "\n",
    "**Characteristics**:\n",
    "- Each document generates complete answer\n",
    "- More consistent (single knowledge source per sequence)\n",
    "- Better for factoid QA\n",
    "\n",
    "### RAG-Token:\n",
    "\n",
    "$$\n",
    "P_{RAG-Token}(y | x) = \\prod_{i=1}^{|y|} \\left( \\sum_{z \\in \\text{top-k}} P(z | x) \\cdot P(y_i | x, z, y_{<i}) \\right)\n",
    "$$\n",
    "\n",
    "**Process**:\n",
    "1. Retrieve top-k documents (same for all tokens)\n",
    "2. For each token: marginalize over documents\n",
    "3. Different documents can contribute to different tokens\n",
    "\n",
    "**Characteristics**:\n",
    "- Can mix information from multiple documents\n",
    "- More flexible generation\n",
    "- Better for long-form generation\n",
    "\n",
    "### Training:\n",
    "\n",
    "**End-to-end**:\n",
    "```\n",
    "Loss = -log P(y* | x)\n",
    "```\n",
    "\n",
    "Gradients flow through:\n",
    "- Generator (BART parameters)\n",
    "- Query encoder (retriever parameters)\n",
    "\n",
    "**Document encoder**: Usually frozen (pre-indexed)\n",
    "\n",
    "### Implementation Details:\n",
    "\n",
    "**From paper**:\n",
    "- Retriever: DPR with BERT-base\n",
    "- Generator: BART-large (400M params)\n",
    "- Knowledge: Wikipedia (21M passages)\n",
    "- Top-k: k=5 or k=10\n",
    "- Index: FAISS for fast retrieval\n",
    "\n",
    "### Results:\n",
    "\n",
    "**Natural Questions (Open)**:\n",
    "- BART (no retrieval): 27.0% EM\n",
    "- RAG-Sequence: 44.5% EM\n",
    "- RAG-Token: 44.1% EM\n",
    "\n",
    "**TriviaQA**:\n",
    "- BART: 50.1%\n",
    "- RAG: 56.8%\n",
    "\n",
    "**WebQuestions**:\n",
    "- BART: 27.6%\n",
    "- RAG: 45.2%\n",
    "\n",
    "### RAG vs Baselines:\n",
    "\n",
    "| Model | Knowledge | Parametric | Performance |\n",
    "|-------|-----------|------------|-------------|\n",
    "| T5-11B | Memorized | ✓ | Good |\n",
    "| REALM | Retrieved | Mixed | Better |\n",
    "| **RAG** | **Retrieved** | **✓** | **Best** |\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "- ✅ **Factual accuracy**: Access to external knowledge\n",
    "- ✅ **Scalability**: Add knowledge without retraining\n",
    "- ✅ **Interpretability**: Can inspect retrieved documents\n",
    "- ✅ **Efficiency**: Smaller models than pure parametric\n",
    "- ✅ **Up-to-date**: Update index, not model weights\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "- ❌ **Retrieval errors**: Wrong docs → wrong answers\n",
    "- ❌ **Latency**: Retrieval adds overhead\n",
    "- ❌ **Index maintenance**: Need to re-encode for updates\n",
    "- ❌ **Memory**: Full document index required\n",
    "\n",
    "### When to Use:\n",
    "\n",
    "**RAG-Sequence**:\n",
    "- Factoid QA\n",
    "- Short answers\n",
    "- When single source is enough\n",
    "\n",
    "**RAG-Token**:\n",
    "- Long-form generation\n",
    "- Multi-hop reasoning\n",
    "- Combining multiple sources\n",
    "\n",
    "### Modern Extensions:\n",
    "\n",
    "- **RETRO** (DeepMind): Retrieve at every layer\n",
    "- **Atlas** (Meta): Improved training\n",
    "- **Toolformer**: Retrieve via API calls\n",
    "- **WebGPT**: Interactive retrieval\n",
    "- **Self-RAG**: Self-reflective retrieval\n",
    "\n",
    "### Production Tips:\n",
    "\n",
    "1. **Hybrid ranking**: Combine retrieval + reranking\n",
    "2. **Cache**: Pre-retrieve for common queries\n",
    "3. **Async**: Retrieve while generating\n",
    "4. **Fallback**: Parametric generation if retrieval fails\n",
    "5. **Monitor**: Track retrieval quality\n",
    "\n",
    "### Applications:\n",
    "\n",
    "- Open-domain QA (Google, Bing)\n",
    "- Chatbots with knowledge bases\n",
    "- Document QA\n",
    "- Fact-checking\n",
    "- Research assistants\n",
    "- Customer support\n",
    "\n",
    "### Key Insight:\n",
    "\n",
    "**RAG = Best of both worlds**\n",
    "- Parametric knowledge (generation capability)\n",
    "- Non-parametric knowledge (external retrieval)\n",
    "- End-to-end differentiable\n",
    "- Practical and effective!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
