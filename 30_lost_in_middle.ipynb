{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 30: Lost in the Middle: How Language Models Use Long Contexts\n",
    "## Nelson F. Liu, Kevin Lin, John Hewitt, et al., Stanford & UW (2023)\n",
    "\n",
    "### The \"Lost in the Middle\" Phenomenon\n",
    "\n",
    "Language models struggle to use information in the middle of long contexts. Performance follows a U-shaped curve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Multi-Document QA Task\n",
    "\n",
    "**Setup**: \n",
    "- Query requires information from ONE document\n",
    "- Multiple documents provided (1 relevant, rest distractors)\n",
    "- **Question**: Does position of relevant document matter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    def __init__(self, content, is_relevant=False):\n",
    "        self.content = content\n",
    "        self.is_relevant = is_relevant\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Doc(relevant={self.is_relevant}): {self.content[:50]}...\"\n",
    "\n",
    "# Create synthetic documents\n",
    "relevant_doc = Document(\n",
    "    \"The Eiffel Tower was completed in 1889 and stands 330 meters tall. \"\n",
    "    \"It was designed by Gustave Eiffel for the 1889 World's Fair in Paris.\",\n",
    "    is_relevant=True\n",
    ")\n",
    "\n",
    "distractor_docs = [\n",
    "    Document(\"The Great Wall of China is over 13,000 miles long and was built over many centuries.\"),\n",
    "    Document(\"The Statue of Liberty was gifted by France to the United States in 1886.\"),\n",
    "    Document(\"Mount Everest is the tallest mountain on Earth at 8,849 meters above sea level.\"),\n",
    "    Document(\"The Amazon River is the largest river by discharge volume in the world.\"),\n",
    "    Document(\"The Sahara Desert is the largest hot desert, covering much of North Africa.\"),\n",
    "    Document(\"The Colosseum in Rome was completed in 80 AD and could hold 50,000 spectators.\"),\n",
    "    Document(\"The Taj Mahal in India was built between 1632 and 1653 as a mausoleum.\"),\n",
    "    Document(\"The Grand Canyon in Arizona is 277 miles long and up to 18 miles wide.\"),\n",
    "    Document(\"The Great Barrier Reef is the world's largest coral reef system.\"),\n",
    "]\n",
    "\n",
    "query = \"When was the Eiffel Tower completed?\"\n",
    "correct_answer = \"1889\"\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Correct answer: {correct_answer}\")\n",
    "print(f\"\\nRelevant document: {relevant_doc.content}\")\n",
    "print(f\"\\nNumber of distractor documents: {len(distractor_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Language Model\n",
    "\n",
    "Simulate attention-based model with position bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLM:\n",
    "    \"\"\"Simplified LM with position bias\"\"\"\n",
    "    def __init__(self, position_bias_type='u_shaped'):\n",
    "        \"\"\"\n",
    "        position_bias_type:\n",
    "        - 'uniform': Equal attention to all positions\n",
    "        - 'u_shaped': High at beginning/end, low in middle\n",
    "        - 'recency': Prefer recent (end) positions\n",
    "        - 'primacy': Prefer early (beginning) positions\n",
    "        \"\"\"\n",
    "        self.position_bias_type = position_bias_type\n",
    "    \n",
    "    def get_position_weights(self, num_positions):\n",
    "        \"\"\"Compute position-based attention weights\"\"\"\n",
    "        positions = np.arange(num_positions)\n",
    "        \n",
    "        if self.position_bias_type == 'uniform':\n",
    "            weights = np.ones(num_positions)\n",
    "        \n",
    "        elif self.position_bias_type == 'u_shaped':\n",
    "            # U-shaped: high at edges, low in middle\n",
    "            normalized_pos = positions / (num_positions - 1)  # 0 to 1\n",
    "            # Quadratic with minimum at 0.5\n",
    "            weights = 4 * (normalized_pos - 0.5) ** 2 + 0.3\n",
    "        \n",
    "        elif self.position_bias_type == 'recency':\n",
    "            # Exponential decay towards beginning\n",
    "            weights = np.exp(positions * 0.2)\n",
    "        \n",
    "        elif self.position_bias_type == 'primacy':\n",
    "            # Exponential decay towards end\n",
    "            weights = np.exp(-positions * 0.2)\n",
    "        \n",
    "        # Normalize\n",
    "        weights = weights / np.sum(weights)\n",
    "        return weights\n",
    "    \n",
    "    def answer_query(self, query, documents):\n",
    "        \"\"\"\n",
    "        Simulate answering query using documents\n",
    "        Returns: probability of finding correct answer\n",
    "        \"\"\"\n",
    "        num_docs = len(documents)\n",
    "        \n",
    "        # Get position weights\n",
    "        position_weights = self.get_position_weights(num_docs)\n",
    "        \n",
    "        # Find relevant document position\n",
    "        relevant_position = None\n",
    "        for i, doc in enumerate(documents):\n",
    "            if doc.is_relevant:\n",
    "                relevant_position = i\n",
    "                break\n",
    "        \n",
    "        if relevant_position is None:\n",
    "            return 0.0  # No relevant document\n",
    "        \n",
    "        # Probability of using relevant document\n",
    "        # Higher weight → more likely to use that document\n",
    "        prob_correct = position_weights[relevant_position]\n",
    "        \n",
    "        return prob_correct\n",
    "\n",
    "# Test different bias types\n",
    "num_docs = 10\n",
    "test_positions = np.arange(num_docs)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "bias_types = ['uniform', 'u_shaped', 'recency', 'primacy']\n",
    "for ax, bias_type in zip(axes, bias_types):\n",
    "    model = SimpleLM(position_bias_type=bias_type)\n",
    "    weights = model.get_position_weights(num_docs)\n",
    "    \n",
    "    ax.bar(test_positions, weights, color='steelblue', edgecolor='black')\n",
    "    ax.set_xlabel('Document Position', fontsize=11)\n",
    "    ax.set_ylabel('Attention Weight', fontsize=11)\n",
    "    ax.set_title(f'{bias_type.replace(\"_\", \" \").title()} Bias', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.set_ylim(0, max(weights) * 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nReal LLMs show U-shaped bias (high at beginning/end, low in middle)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Position Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_all_positions(model, query, relevant_doc, distractor_docs):\n",
    "    \"\"\"\n",
    "    Test performance with relevant document at each position\n",
    "    \"\"\"\n",
    "    num_positions = len(distractor_docs) + 1\n",
    "    accuracies = []\n",
    "    \n",
    "    for pos in range(num_positions):\n",
    "        # Create document list with relevant doc at position 'pos'\n",
    "        docs = distractor_docs[:pos] + [relevant_doc] + distractor_docs[pos:]\n",
    "        docs = docs[:num_positions]  # Keep fixed length\n",
    "        \n",
    "        # Get model's probability of answering correctly\n",
    "        prob_correct = model.answer_query(query, docs)\n",
    "        accuracies.append(prob_correct)\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "# Test U-shaped bias (realistic)\n",
    "model_realistic = SimpleLM(position_bias_type='u_shaped')\n",
    "accuracies_realistic = test_all_positions(model_realistic, query, relevant_doc, distractor_docs)\n",
    "\n",
    "# Test uniform (ideal)\n",
    "model_ideal = SimpleLM(position_bias_type='uniform')\n",
    "accuracies_ideal = test_all_positions(model_ideal, query, relevant_doc, distractor_docs)\n",
    "\n",
    "# Plot\n",
    "positions = np.arange(len(accuracies_realistic))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(positions, accuracies_realistic, 'o-', linewidth=3, markersize=10, \n",
    "        label='Realistic (U-shaped bias)', color='crimson')\n",
    "plt.plot(positions, accuracies_ideal, 's--', linewidth=2, markersize=8, \n",
    "        label='Ideal (No bias)', color='green', alpha=0.6)\n",
    "\n",
    "# Mark beginning and end\n",
    "plt.axvline(x=0, color='blue', linestyle=':', alpha=0.5, linewidth=2, label='Beginning')\n",
    "plt.axvline(x=len(positions)-1, color='purple', linestyle=':', alpha=0.5, linewidth=2, label='End')\n",
    "\n",
    "# Mark middle region\n",
    "middle_start = len(positions) // 4\n",
    "middle_end = 3 * len(positions) // 4\n",
    "plt.axvspan(middle_start, middle_end, alpha=0.2, color='red', label='Middle (worst)')\n",
    "\n",
    "plt.xlabel('Position of Relevant Document', fontsize=13)\n",
    "plt.ylabel('Accuracy', fontsize=13)\n",
    "plt.title('Lost in the Middle: Performance vs Position', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Stats\n",
    "beginning_acc = accuracies_realistic[0]\n",
    "middle_acc = np.mean(accuracies_realistic[middle_start:middle_end])\n",
    "end_acc = accuracies_realistic[-1]\n",
    "\n",
    "print(f\"\\nPerformance Analysis:\")\n",
    "print(f\"Beginning (pos 0): {beginning_acc:.1%}\")\n",
    "print(f\"Middle (pos {middle_start}-{middle_end}): {middle_acc:.1%}\")\n",
    "print(f\"End (pos {len(positions)-1}): {end_acc:.1%}\")\n",
    "print(f\"\\nMiddle penalty: -{(beginning_acc - middle_acc)/beginning_acc:.1%} relative to beginning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impact of Context Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_varying_lengths(model, query, relevant_doc, distractor_docs, lengths):\n",
    "    \"\"\"\n",
    "    Test how performance changes with context length\n",
    "    \"\"\"\n",
    "    results = {'beginning': [], 'middle': [], 'end': []}\n",
    "    \n",
    "    for length in lengths:\n",
    "        # Use subset of distractors\n",
    "        current_distractors = distractor_docs[:length-1]\n",
    "        \n",
    "        # Test three positions: beginning, middle, end\n",
    "        positions = {\n",
    "            'beginning': 0,\n",
    "            'middle': length // 2,\n",
    "            'end': length - 1\n",
    "        }\n",
    "        \n",
    "        for pos_name, pos in positions.items():\n",
    "            docs = current_distractors[:pos] + [relevant_doc] + current_distractors[pos:]\n",
    "            docs = docs[:length]\n",
    "            \n",
    "            acc = model.answer_query(query, docs)\n",
    "            results[pos_name].append(acc)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test different context lengths\n",
    "lengths = [3, 5, 7, 9, 10]\n",
    "results = test_varying_lengths(model_realistic, query, relevant_doc, distractor_docs, lengths)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(lengths, results['beginning'], 'o-', linewidth=3, markersize=10, \n",
    "        label='Beginning', color='blue')\n",
    "plt.plot(lengths, results['middle'], 's-', linewidth=3, markersize=10, \n",
    "        label='Middle', color='red')\n",
    "plt.plot(lengths, results['end'], '^-', linewidth=3, markersize=10, \n",
    "        label='End', color='purple')\n",
    "\n",
    "plt.xlabel('Number of Documents', fontsize=13)\n",
    "plt.ylabel('Accuracy', fontsize=13)\n",
    "plt.title('Performance Degradation with Context Length', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLonger contexts → worse performance (especially in middle!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordering Strategies for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_documents(documents, relevance_scores, strategy='default'):\n",
    "    \"\"\"\n",
    "    Order documents according to strategy\n",
    "    \n",
    "    Strategies:\n",
    "    - 'default': Keep retrieval order\n",
    "    - 'most_relevant_first': Put best documents at beginning\n",
    "    - 'most_relevant_edges': Put best at beginning & end\n",
    "    - 'reverse': Reverse retrieval order\n",
    "    \"\"\"\n",
    "    indices = np.arange(len(documents))\n",
    "    \n",
    "    if strategy == 'default':\n",
    "        return documents\n",
    "    \n",
    "    elif strategy == 'most_relevant_first':\n",
    "        # Sort by relevance (descending)\n",
    "        sorted_indices = np.argsort(relevance_scores)[::-1]\n",
    "        return [documents[i] for i in sorted_indices]\n",
    "    \n",
    "    elif strategy == 'most_relevant_edges':\n",
    "        # Put most relevant at beginning and end\n",
    "        sorted_indices = np.argsort(relevance_scores)[::-1]\n",
    "        \n",
    "        # Interleave: best at edges, worst in middle\n",
    "        ordered = []\n",
    "        for i in range(len(documents) // 2):\n",
    "            ordered.append(documents[sorted_indices[i]])  # High relevance\n",
    "        for i in range(len(documents) // 2, len(documents)):\n",
    "            ordered.append(documents[sorted_indices[i]])  # Low relevance\n",
    "        \n",
    "        # Reverse second half to put high at end\n",
    "        mid = len(ordered) // 2\n",
    "        return ordered[:mid] + ordered[mid:][::-1]\n",
    "    \n",
    "    elif strategy == 'reverse':\n",
    "        return documents[::-1]\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Simulate retrieval scores\n",
    "num_test_docs = 10\n",
    "test_docs = [relevant_doc] + distractor_docs[:num_test_docs-1]\n",
    "\n",
    "# Relevance scores (relevant doc gets high score)\n",
    "relevance_scores = np.random.rand(num_test_docs) * 0.5\n",
    "relevance_scores[0] = 0.95  # Relevant doc has high score\n",
    "\n",
    "# Shuffle to simulate retrieval\n",
    "shuffle_idx = np.random.permutation(num_test_docs)\n",
    "test_docs = [test_docs[i] for i in shuffle_idx]\n",
    "relevance_scores = relevance_scores[shuffle_idx]\n",
    "\n",
    "# Test different strategies\n",
    "strategies = ['default', 'most_relevant_first', 'most_relevant_edges']\n",
    "strategy_accuracies = {}\n",
    "\n",
    "for strategy in strategies:\n",
    "    ordered = order_documents(test_docs, relevance_scores, strategy)\n",
    "    acc = model_realistic.answer_query(query, ordered)\n",
    "    strategy_accuracies[strategy] = acc\n",
    "    \n",
    "    # Find position of relevant doc\n",
    "    rel_pos = next(i for i, doc in enumerate(ordered) if doc.is_relevant)\n",
    "    print(f\"\\n{strategy:25s}: Relevant doc at position {rel_pos:2d}, Accuracy: {acc:.1%}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(range(len(strategies)), \n",
    "              [strategy_accuracies[s] for s in strategies],\n",
    "              color=['lightcoral', 'lightblue', 'lightgreen'],\n",
    "              edgecolor='black', linewidth=2)\n",
    "\n",
    "plt.xticks(range(len(strategies)), \n",
    "          [s.replace('_', '\\n').title() for s in strategies],\n",
    "          fontsize=11)\n",
    "plt.ylabel('Accuracy', fontsize=13)\n",
    "plt.title('Document Ordering Strategies', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels\n",
    "for bar, strategy in zip(bars, strategies):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{strategy_accuracies[strategy]:.1%}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RECOMMENDATION: Put most important documents at edges!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate attention patterns for different context lengths\n",
    "context_lengths = [10, 20, 30]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "for ax, length in zip(axes, context_lengths):\n",
    "    # Generate attention weights (U-shaped)\n",
    "    positions = np.arange(length)\n",
    "    normalized = positions / (length - 1)\n",
    "    attention = 4 * (normalized - 0.5) ** 2 + 0.3\n",
    "    attention = attention / np.sum(attention)\n",
    "    \n",
    "    # Plot\n",
    "    ax.bar(positions, attention, color='steelblue', edgecolor='black', linewidth=1)\n",
    "    ax.set_xlabel('Position', fontsize=11)\n",
    "    ax.set_ylabel('Attention Weight', fontsize=11)\n",
    "    ax.set_title(f'Context Length = {length}', fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Highlight middle region\n",
    "    middle_start = length // 4\n",
    "    middle_end = 3 * length // 4\n",
    "    ax.axvspan(middle_start, middle_end, alpha=0.2, color='red')\n",
    "\n",
    "plt.suptitle('Attention Patterns: Lost in the Middle', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAs context grows, middle positions get even less attention!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### The Lost in the Middle Phenomenon:\n",
    "\n",
    "**Observation**: Language models show **U-shaped performance curve**\n",
    "- ✅ High accuracy when relevant info is at **beginning**\n",
    "- ✅ High accuracy when relevant info is at **end**  \n",
    "- ❌ **Low accuracy** when relevant info is in the **middle**\n",
    "\n",
    "### Why Does This Happen?\n",
    "\n",
    "**Hypotheses**:\n",
    "\n",
    "1. **Attention patterns**:\n",
    "   - Self-attention naturally focuses on recent tokens (recency bias)\n",
    "   - Also focuses on early tokens (primacy bias)\n",
    "   - Middle tokens receive less attention\n",
    "\n",
    "2. **Training distribution**:\n",
    "   - Most training documents are short\n",
    "   - Long contexts are rare in pre-training\n",
    "   - Models haven't learned to use middle well\n",
    "\n",
    "3. **Causal masking**:\n",
    "   - Decoder models can't \"look ahead\"\n",
    "   - Information in middle may be \"overwritten\" by later tokens\n",
    "\n",
    "### Experimental Findings:\n",
    "\n",
    "**From the paper**:\n",
    "\n",
    "**Multi-document QA**:\n",
    "- Relevant doc at position 1 (beginning): ~90% accuracy\n",
    "- Relevant doc at position 5 (middle): ~60% accuracy  \n",
    "- Relevant doc at position 10 (end): ~85% accuracy\n",
    "\n",
    "**Effect of context length**:\n",
    "- 10 documents: Middle penalty ~30%\n",
    "- 20 documents: Middle penalty ~40%\n",
    "- 30 documents: Middle penalty ~50%\n",
    "\n",
    "**Models tested**:\n",
    "- GPT-3.5-turbo: Strong U-shaped bias\n",
    "- Claude: Strong U-shaped bias\n",
    "- GPT-4: Mitigated but still present\n",
    "- Open-source LLMs: Even stronger bias\n",
    "\n",
    "### Position Bias Formula:\n",
    "\n",
    "Performance at position $p$ (normalized 0-1):\n",
    "$$\n",
    "\\text{Accuracy}(p) \\propto 4(p - 0.5)^2 + c\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- Minimum at $p = 0.5$ (middle)\n",
    "- Maximum at $p = 0$ and $p = 1$ (edges)\n",
    "- $c$ is baseline performance\n",
    "\n",
    "### Implications for RAG Systems:\n",
    "\n",
    "**Problem**:\n",
    "```\n",
    "Retriever returns: [Doc1, Doc2, ..., Doc20]\n",
    "                    (sorted by relevance score)\n",
    "\n",
    "If most relevant doc is in middle → poor performance!\n",
    "```\n",
    "\n",
    "**Solutions**:\n",
    "\n",
    "1. **Reorder retrieved documents**:\n",
    "   - Put most relevant at beginning\n",
    "   - Or interleave: best at edges, worst in middle\n",
    "\n",
    "2. **Limit context length**:\n",
    "   - Use fewer, more relevant documents\n",
    "   - Top-3 or top-5 instead of top-20\n",
    "\n",
    "3. **Chunking**:\n",
    "   - Process long contexts in smaller chunks\n",
    "   - Aggregate results\n",
    "\n",
    "4. **Explicit attention**:\n",
    "   - Fine-tune model to attend to middle\n",
    "   - Add position embeddings that counter bias\n",
    "\n",
    "### Document Ordering Strategies:\n",
    "\n",
    "| Strategy | Description | Performance |\n",
    "|----------|-------------|-------------|\n",
    "| Retrieval order | Keep as retrieved | Baseline |\n",
    "| Most relevant first | Best at beginning | Good |\n",
    "| Most relevant edges | Best at begin & end | **Best** |\n",
    "| Reverse | Flip retrieval order | Varies |\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Short contexts** when possible\n",
    "2. **Important info at edges** (beginning or end)\n",
    "3. **Rerank** documents before passing to LLM\n",
    "4. **Chunk** very long contexts\n",
    "5. **Test** position sensitivity for your model\n",
    "\n",
    "### Code Example (Reordering):\n",
    "\n",
    "```python\n",
    "def reorder_for_llm(docs, scores):\n",
    "    \"\"\"Put most relevant at edges\"\"\"\n",
    "    sorted_idx = np.argsort(scores)[::-1]\n",
    "    \n",
    "    # Interleave high and low relevance\n",
    "    reordered = []\n",
    "    for i in range(len(docs) // 2):\n",
    "        reordered.append(docs[sorted_idx[i]])  # High\n",
    "    for i in range(len(docs) // 2, len(docs)):\n",
    "        reordered.append(docs[sorted_idx[i]])  # Low\n",
    "    \n",
    "    # Move best to end as well\n",
    "    mid = len(reordered) // 2\n",
    "    return reordered[:mid] + reordered[mid:][::-1]\n",
    "```\n",
    "\n",
    "### Mitigation Strategies:\n",
    "\n",
    "**During training**:\n",
    "- Include long-context examples\n",
    "- Explicitly supervise middle positions\n",
    "- Use position-aware objectives\n",
    "\n",
    "**During inference**:\n",
    "- Reorder documents strategically\n",
    "- Use multiple passes (process subsets)\n",
    "- Explicit prompting: \"Focus on all documents equally\"\n",
    "\n",
    "**Architecture changes**:\n",
    "- Sparse attention patterns\n",
    "- Hierarchical processing\n",
    "- Retrieval-augmented attention\n",
    "\n",
    "### Future Directions:\n",
    "\n",
    "- **Position-invariant models**: Train to ignore position bias\n",
    "- **Adaptive attention**: Learn to focus on relevant parts\n",
    "- **Chunked processing**: Process in overlapping windows\n",
    "- **Multi-pass reasoning**: Multiple reads of context\n",
    "\n",
    "### Takeaway Message:\n",
    "\n",
    "```\n",
    "⚠️  WARNING: Don't assume LLMs use all context equally!\n",
    "\n",
    "✅  DO: Test position sensitivity\n",
    "✅  DO: Put important info at edges  \n",
    "✅  DO: Keep contexts short when possible\n",
    "❌  DON'T: Assume middle positions work well\n",
    "❌  DON'T: Blindly concatenate many documents\n",
    "```\n",
    "\n",
    "### Impact:\n",
    "\n",
    "This paper revealed a critical limitation of current LLMs and changed how we think about:\n",
    "- RAG system design\n",
    "- Long-context evaluation\n",
    "- Document ordering for QA\n",
    "- Prompt engineering with multiple sources\n",
    "\n",
    "**Remember**: Even with 100k+ context windows, position matters!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
