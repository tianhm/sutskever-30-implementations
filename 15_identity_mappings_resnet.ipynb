{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 15: Identity Mappings in Deep Residual Networks\n",
    "## Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun (2016)\n",
    "\n",
    "### Pre-activation ResNet\n",
    "\n",
    "Improved residual blocks with better gradient flow. Key insight: move activation BEFORE convolution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original ResNet Block\n",
    "\n",
    "```\n",
    "x → Conv → BN → ReLU → Conv → BN → (+) → ReLU → output\n",
    "    ↓                                  ↑\n",
    "    └──────────── identity ────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def batch_norm_1d(x, gamma=1.0, beta=0.0, eps=1e-5):\n",
    "    \"\"\"Simplified batch normalization for 1D\"\"\"\n",
    "    mean = np.mean(x)\n",
    "    var = np.var(x)\n",
    "    x_normalized = (x - mean) / np.sqrt(var + eps)\n",
    "    return gamma * x_normalized + beta\n",
    "\n",
    "class OriginalResidualBlock:\n",
    "    \"\"\"Original ResNet block (post-activation)\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "        # Two layers\n",
    "        self.W1 = np.random.randn(dim, dim) * 0.01\n",
    "        self.W2 = np.random.randn(dim, dim) * 0.01\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Original: x → Conv → BN → ReLU → Conv → BN → (+x) → ReLU\n",
    "        \"\"\"\n",
    "        # First conv-bn-relu\n",
    "        out = np.dot(self.W1, x)\n",
    "        out = batch_norm_1d(out)\n",
    "        out = relu(out)\n",
    "        \n",
    "        # Second conv-bn\n",
    "        out = np.dot(self.W2, out)\n",
    "        out = batch_norm_1d(out)\n",
    "        \n",
    "        # Add identity (residual connection)\n",
    "        out = out + x\n",
    "        \n",
    "        # Final ReLU (post-activation)\n",
    "        out = relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Test\n",
    "original_block = OriginalResidualBlock(dim=8)\n",
    "x = np.random.randn(8)\n",
    "output_original = original_block.forward(x)\n",
    "\n",
    "print(f\"Input: {x[:4]}...\")\n",
    "print(f\"Original ResNet output: {output_original[:4]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-activation ResNet Block\n",
    "\n",
    "```\n",
    "x → BN → ReLU → Conv → BN → ReLU → Conv → (+) → output\n",
    "    ↓                                       ↑\n",
    "    └──────────── identity ─────────────────┘\n",
    "```\n",
    "\n",
    "**Key difference**: Activation BEFORE convolution, clean identity path!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreActivationResidualBlock:\n",
    "    \"\"\"Pre-activation ResNet block (improved)\"\"\"\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "        self.W1 = np.random.randn(dim, dim) * 0.01\n",
    "        self.W2 = np.random.randn(dim, dim) * 0.01\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Pre-activation: x → BN → ReLU → Conv → BN → ReLU → Conv → (+x)\n",
    "        \"\"\"\n",
    "        # First bn-relu-conv\n",
    "        out = batch_norm_1d(x)\n",
    "        out = relu(out)\n",
    "        out = np.dot(self.W1, out)\n",
    "        \n",
    "        # Second bn-relu-conv\n",
    "        out = batch_norm_1d(out)\n",
    "        out = relu(out)\n",
    "        out = np.dot(self.W2, out)\n",
    "        \n",
    "        # Add identity (NO activation after!)\n",
    "        out = out + x\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Test\n",
    "preact_block = PreActivationResidualBlock(dim=8)\n",
    "output_preact = preact_block.forward(x)\n",
    "\n",
    "print(f\"\\nPre-activation ResNet output: {output_preact[:4]}...\")\n",
    "print(f\"\\nKey difference: Clean identity path (no ReLU after addition)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Flow Analysis\n",
    "\n",
    "Why pre-activation is better:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_flow(block_type, num_layers=10, input_dim=8):\n",
    "    \"\"\"\n",
    "    Simulate gradient flow through stacked residual blocks\n",
    "    \"\"\"\n",
    "    x = np.random.randn(input_dim)\n",
    "    \n",
    "    # Create blocks\n",
    "    if block_type == 'original':\n",
    "        blocks = [OriginalResidualBlock(input_dim) for _ in range(num_layers)]\n",
    "    else:\n",
    "        blocks = [PreActivationResidualBlock(input_dim) for _ in range(num_layers)]\n",
    "    \n",
    "    # Forward pass\n",
    "    activations = [x]\n",
    "    current = x\n",
    "    for block in blocks:\n",
    "        current = block.forward(current)\n",
    "        activations.append(current.copy())\n",
    "    \n",
    "    # Simulate backward pass (simplified gradient flow)\n",
    "    grad = np.ones(input_dim)  # Gradient from loss\n",
    "    gradients = [grad]\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        # For residual blocks: gradient splits into identity + residual path\n",
    "        # Pre-activation has cleaner gradient flow\n",
    "        \n",
    "        if block_type == 'original':\n",
    "            # Post-activation: gradient affected by ReLU derivative\n",
    "            # Simplified: some gradient is killed by ReLU\n",
    "            grad_through_residual = grad * np.random.uniform(0.5, 1.0, input_dim)\n",
    "            grad = grad + grad_through_residual  # Identity + residual\n",
    "        else:\n",
    "            # Pre-activation: clean identity path\n",
    "            grad_through_residual = grad * np.random.uniform(0.7, 1.0, input_dim)\n",
    "            grad = grad + grad_through_residual  # Better gradient flow\n",
    "        \n",
    "        gradients.append(grad.copy())\n",
    "    \n",
    "    return activations, gradients\n",
    "\n",
    "# Compare gradient flow\n",
    "_, grad_original = compute_gradient_flow('original', num_layers=20)\n",
    "_, grad_preact = compute_gradient_flow('preact', num_layers=20)\n",
    "\n",
    "# Compute gradient magnitudes\n",
    "grad_mag_original = [np.linalg.norm(g) for g in grad_original]\n",
    "grad_mag_preact = [np.linalg.norm(g) for g in grad_preact]\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(grad_mag_original, 'o-', label='Original ResNet (post-activation)', linewidth=2)\n",
    "plt.plot(grad_mag_preact, 's-', label='Pre-activation ResNet', linewidth=2)\n",
    "plt.xlabel('Layer (from output to input)', fontsize=12)\n",
    "plt.ylabel('Gradient Magnitude', fontsize=12)\n",
    "plt.title('Gradient Flow Comparison', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Original ResNet gradient at input: {grad_mag_original[-1]:.2f}\")\n",
    "print(f\"Pre-activation gradient at input: {grad_mag_preact[-1]:.2f}\")\n",
    "print(f\"\\nPre-activation maintains stronger gradients!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Activation Placements\n",
    "\n",
    "The paper analyzes various placement options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize different architectures\n",
    "architectures = [\n",
    "    {\n",
    "        'name': 'Original',\n",
    "        'structure': 'x → Conv → BN → ReLU → Conv → BN → (+x) → ReLU',\n",
    "        'identity': 'Blocked by ReLU',\n",
    "        'score': '★★★☆☆'\n",
    "    },\n",
    "    {\n",
    "        'name': 'BN after addition',\n",
    "        'structure': 'x → Conv → BN → ReLU → Conv → BN → (+x) → BN → ReLU',\n",
    "        'identity': 'Blocked by BN & ReLU',\n",
    "        'score': '★★☆☆☆'\n",
    "    },\n",
    "    {\n",
    "        'name': 'ReLU before addition',\n",
    "        'structure': 'x → BN → ReLU → Conv → BN → ReLU → Conv → ReLU → (+x)',\n",
    "        'identity': 'Blocked by ReLU',\n",
    "        'score': '★★☆☆☆'\n",
    "    },\n",
    "    {\n",
    "        'name': 'Full pre-activation',\n",
    "        'structure': 'x → BN → ReLU → Conv → BN → ReLU → Conv → (+x)',\n",
    "        'identity': 'CLEAN! ✓',\n",
    "        'score': '★★★★★'\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESIDUAL BLOCK ARCHITECTURES COMPARISON\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for i, arch in enumerate(architectures, 1):\n",
    "    print(f\"{i}. {arch['name']:20s} {arch['score']}\")\n",
    "    print(f\"   Structure: {arch['structure']}\")\n",
    "    print(f\"   Identity path: {arch['identity']}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"WINNER: Full pre-activation (BN → ReLU → Conv)\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Network Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepResNet:\n",
    "    \"\"\"Stack of residual blocks\"\"\"\n",
    "    def __init__(self, dim, num_blocks, block_type='preact'):\n",
    "        self.blocks = []\n",
    "        for _ in range(num_blocks):\n",
    "            if block_type == 'preact':\n",
    "                self.blocks.append(PreActivationResidualBlock(dim))\n",
    "            else:\n",
    "                self.blocks.append(OriginalResidualBlock(dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        activations = [x]\n",
    "        for block in self.blocks:\n",
    "            x = block.forward(x)\n",
    "            activations.append(x.copy())\n",
    "        return x, activations\n",
    "\n",
    "# Compare deep networks\n",
    "depth = 50\n",
    "dim = 16\n",
    "x_input = np.random.randn(dim)\n",
    "\n",
    "net_original = DeepResNet(dim, depth, 'original')\n",
    "net_preact = DeepResNet(dim, depth, 'preact')\n",
    "\n",
    "out_original, acts_original = net_original.forward(x_input)\n",
    "out_preact, acts_preact = net_preact.forward(x_input)\n",
    "\n",
    "# Compute activation statistics\n",
    "norms_original = [np.linalg.norm(a) for a in acts_original]\n",
    "norms_preact = [np.linalg.norm(a) for a in acts_preact]\n",
    "\n",
    "# Plot activation norms\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Activation magnitudes\n",
    "ax1.plot(norms_original, label='Original ResNet', linewidth=2)\n",
    "ax1.plot(norms_preact, label='Pre-activation ResNet', linewidth=2)\n",
    "ax1.set_xlabel('Layer', fontsize=12)\n",
    "ax1.set_ylabel('Activation Magnitude', fontsize=12)\n",
    "ax1.set_title(f'Activation Flow (Depth={depth})', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Activation heatmaps\n",
    "acts_matrix_original = np.array(acts_original).T\n",
    "acts_matrix_preact = np.array(acts_preact).T\n",
    "\n",
    "im = ax2.imshow(acts_matrix_preact - acts_matrix_original, cmap='RdBu', aspect='auto')\n",
    "ax2.set_xlabel('Layer', fontsize=12)\n",
    "ax2.set_ylabel('Feature Dimension', fontsize=12)\n",
    "ax2.set_title('Difference (Pre-act - Original)', fontsize=14)\n",
    "plt.colorbar(im, ax=ax2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOriginal ResNet final norm: {norms_original[-1]:.4f}\")\n",
    "print(f\"Pre-activation final norm: {norms_preact[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity Mapping Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_identity_mapping(block, num_tests=100):\n",
    "    \"\"\"\n",
    "    Test how well the block can learn identity mapping\n",
    "    (When residual path learns zero, output should equal input)\n",
    "    \"\"\"\n",
    "    # Zero out weights (residual path learns nothing)\n",
    "    block.W1 = np.zeros_like(block.W1)\n",
    "    block.W2 = np.zeros_like(block.W2)\n",
    "    \n",
    "    errors = []\n",
    "    for _ in range(num_tests):\n",
    "        x = np.random.randn(block.dim)\n",
    "        y = block.forward(x)\n",
    "        error = np.linalg.norm(y - x)\n",
    "        errors.append(error)\n",
    "    \n",
    "    return np.mean(errors), np.std(errors)\n",
    "\n",
    "# Test both block types\n",
    "original_test = OriginalResidualBlock(dim=8)\n",
    "preact_test = PreActivationResidualBlock(dim=8)\n",
    "\n",
    "mean_err_original, std_err_original = test_identity_mapping(original_test)\n",
    "mean_err_preact, std_err_preact = test_identity_mapping(preact_test)\n",
    "\n",
    "print(\"\\nIdentity Mapping Test (residual path = 0):\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Original ResNet error: {mean_err_original:.6f} ± {std_err_original:.6f}\")\n",
    "print(f\"Pre-activation error:  {mean_err_preact:.6f} ± {std_err_preact:.6f}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nPre-activation has {'BETTER' if mean_err_preact < mean_err_original else 'WORSE'} identity mapping!\")\n",
    "print(\"(Lower error = cleaner identity path)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Architecture Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visual comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "def draw_block(ax, title, is_preact=False):\n",
    "    ax.set_xlim(0, 10)\n",
    "    ax.set_ylim(0, 12)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Identity path (left)\n",
    "    ax.plot([1, 1], [1, 11], 'b-', linewidth=4, label='Identity path')\n",
    "    ax.arrow(1, 10.5, 0, -0.3, head_width=0.3, head_length=0.2, fc='blue', ec='blue')\n",
    "    \n",
    "    # Residual path (right)\n",
    "    y_pos = 11\n",
    "    \n",
    "    if is_preact:\n",
    "        # Pre-activation: BN → ReLU → Conv → BN → ReLU → Conv\n",
    "        operations = ['BN', 'ReLU', 'Conv', 'BN', 'ReLU', 'Conv']\n",
    "        colors = ['lightgreen', 'lightyellow', 'lightblue', 'lightgreen', 'lightyellow', 'lightblue']\n",
    "    else:\n",
    "        # Original: Conv → BN → ReLU → Conv → BN\n",
    "        operations = ['Conv', 'BN', 'ReLU', 'Conv', 'BN', 'ReLU*']\n",
    "        colors = ['lightblue', 'lightgreen', 'lightyellow', 'lightblue', 'lightgreen', 'lightcoral']\n",
    "    \n",
    "    for i, (op, color) in enumerate(zip(operations, colors)):\n",
    "        y = y_pos - i * 1.5\n",
    "        \n",
    "        # Draw box\n",
    "        width = 2\n",
    "        height = 1\n",
    "        ax.add_patch(plt.Rectangle((6-width/2, y-height/2), width, height, \n",
    "                                   fill=True, color=color, ec='black', linewidth=2))\n",
    "        ax.text(6, y, op, ha='center', va='center', fontsize=11, fontweight='bold')\n",
    "        \n",
    "        # Draw arrow to next\n",
    "        if i < len(operations) - 1:\n",
    "            ax.arrow(6, y-height/2-0.1, 0, -0.3, head_width=0.2, head_length=0.1, \n",
    "                    fc='black', ec='black', linewidth=1.5)\n",
    "    \n",
    "    # Addition\n",
    "    add_y = y_pos - len(operations) * 1.5\n",
    "    ax.plot([1, 6], [add_y, add_y], 'k-', linewidth=2)\n",
    "    ax.scatter([3.5], [add_y], s=500, c='white', edgecolors='black', linewidths=3, zorder=5)\n",
    "    ax.text(3.5, add_y, '+', ha='center', va='center', fontsize=20, fontweight='bold', zorder=6)\n",
    "    \n",
    "    # Output arrow\n",
    "    ax.arrow(3.5, add_y-0.3, 0, -0.5, head_width=0.3, head_length=0.2, \n",
    "            fc='green', ec='green', linewidth=3)\n",
    "    ax.text(3.5, add_y-1.2, 'Output', ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Input\n",
    "    ax.text(1, 11.5, 'Input', ha='center', fontsize=12, fontweight='bold')\n",
    "    ax.text(6, 11.5, 'Input', ha='center', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Annotations\n",
    "    if not is_preact:\n",
    "        ax.text(8.5, add_y, 'ReLU* blocks\\nidentity!', fontsize=10, color='red', \n",
    "               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    else:\n",
    "        ax.text(8.5, add_y, 'Clean\\nidentity!', fontsize=10, color='green',\n",
    "               bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.5))\n",
    "\n",
    "draw_block(axes[0], 'Original ResNet (Post-activation)', is_preact=False)\n",
    "draw_block(axes[1], 'Pre-activation ResNet (Improved)', is_preact=True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### The Identity Mapping Problem:\n",
    "\n",
    "In original ResNet:\n",
    "```\n",
    "y = ReLU(F(x) + x)\n",
    "```\n",
    "The ReLU **after addition blocks** the identity path!\n",
    "\n",
    "### Pre-activation Solution:\n",
    "\n",
    "```\n",
    "y = F'(x) + x\n",
    "```\n",
    "where F'(x) = Conv(ReLU(BN(Conv(ReLU(BN(x))))))\n",
    "\n",
    "**Clean identity path** → better gradient flow!\n",
    "\n",
    "### Key Changes:\n",
    "\n",
    "1. **Move BN before Conv**: `x → BN → ReLU → Conv`\n",
    "2. **Remove final ReLU**: No activation after addition\n",
    "3. **Result**: Identity path is truly identity\n",
    "\n",
    "### Gradient Flow:\n",
    "\n",
    "**Original**:\n",
    "```\n",
    "∂L/∂x = ∂L/∂y · (∂F/∂x + I) · ∂ReLU/∂y\n",
    "```\n",
    "ReLU derivative kills gradients!\n",
    "\n",
    "**Pre-activation**:\n",
    "```\n",
    "∂L/∂x = ∂L/∂y · (∂F'/∂x + I)\n",
    "```\n",
    "Clean gradient flow through identity!\n",
    "\n",
    "### Benefits:\n",
    "\n",
    "- ✅ **Better gradient flow**: No blocking on identity path\n",
    "- ✅ **Easier optimization**: Can train deeper networks (1000+ layers)\n",
    "- ✅ **Better accuracy**: Small but consistent improvement\n",
    "- ✅ **Regularization**: BN before Conv acts as regularizer\n",
    "\n",
    "### Comparison:\n",
    "\n",
    "| Architecture | Identity Path | Gradient Flow | Performance |\n",
    "|--------------|---------------|---------------|-------------|\n",
    "| Original ResNet | Blocked by ReLU | Good | ★★★★☆ |\n",
    "| Pre-activation | **Clean** | **Better** | ★★★★★ |\n",
    "\n",
    "### Implementation Tips:\n",
    "\n",
    "1. Use pre-activation for very deep networks (>50 layers)\n",
    "2. Keep original ResNet for shallower networks (backward compatibility)\n",
    "3. First layer can keep post-activation (no identity yet)\n",
    "4. Last layer needs post-activation for final output\n",
    "\n",
    "### Results:\n",
    "\n",
    "- CIFAR-10: 1001-layer network trained successfully!\n",
    "- ImageNet: Consistent improvements over original ResNet\n",
    "- Enabled training of 1000+ layer networks\n",
    "\n",
    "### Why It Matters:\n",
    "\n",
    "This paper showed that **architecture details matter**. Small changes (moving BN/ReLU) can have significant impact on trainability and performance. It's a key example of iterative improvement in deep learning research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
