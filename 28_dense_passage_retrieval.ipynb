{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 28: Dense Passage Retrieval for Open-Domain Question Answering\n",
    "## Vladimir Karpukhin, Barlas Oğuz, Sewon Min, et al., Meta AI (2020)\n",
    "\n",
    "### Dense Passage Retrieval (DPR)\n",
    "\n",
    "Learn dense embeddings for questions and passages. Retrieve via similarity in embedding space. Beats BM25!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dual Encoder Architecture\n",
    "\n",
    "```\n",
    "Question → Encoder_Q → q (dense vector)\n",
    "Passage  → Encoder_P → p (dense vector)\n",
    "\n",
    "Similarity: sim(q, p) = q · p  (dot product)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTextEncoder:\n",
    "    \"\"\"Simplified text encoder (in practice: use BERT)\"\"\"\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embeddings\n",
    "        self.embeddings = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "        \n",
    "        # Simple RNN weights\n",
    "        self.W_xh = np.random.randn(hidden_dim, embedding_dim) * 0.01\n",
    "        self.W_hh = np.random.randn(hidden_dim, hidden_dim) * 0.01\n",
    "        self.b_h = np.zeros((hidden_dim, 1))\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_out = np.random.randn(hidden_dim, hidden_dim) * 0.01\n",
    "    \n",
    "    def encode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Encode sequence of token IDs to dense vector\n",
    "        Returns: dense embedding (hidden_dim,)\n",
    "        \"\"\"\n",
    "        h = np.zeros((self.hidden_dim, 1))\n",
    "        \n",
    "        # Process tokens\n",
    "        for token_id in token_ids:\n",
    "            # Lookup embedding\n",
    "            x = self.embeddings[token_id].reshape(-1, 1)\n",
    "            \n",
    "            # RNN step\n",
    "            h = np.tanh(np.dot(self.W_xh, x) + np.dot(self.W_hh, h) + self.b_h)\n",
    "        \n",
    "        # Final representation (CLS-like)\n",
    "        output = np.dot(self.W_out, h).flatten()\n",
    "        \n",
    "        # L2 normalize for cosine similarity\n",
    "        output = output / (np.linalg.norm(output) + 1e-8)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Create encoders\n",
    "vocab_size = 1000\n",
    "embedding_dim = 64\n",
    "hidden_dim = 128\n",
    "\n",
    "question_encoder = SimpleTextEncoder(vocab_size, embedding_dim, hidden_dim)\n",
    "passage_encoder = SimpleTextEncoder(vocab_size, embedding_dim, hidden_dim)\n",
    "\n",
    "# Test\n",
    "test_tokens = [10, 25, 37, 42]\n",
    "q_emb = question_encoder.encode(test_tokens)\n",
    "p_emb = passage_encoder.encode(test_tokens)\n",
    "\n",
    "print(f\"Question embedding shape: {q_emb.shape}\")\n",
    "print(f\"Passage embedding shape: {p_emb.shape}\")\n",
    "print(f\"Similarity (dot product): {np.dot(q_emb, p_emb):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic QA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    \"\"\"Simple word tokenizer\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word_to_id = {}\n",
    "        self.id_to_word = {}\n",
    "        self.next_id = 0\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Convert text to token IDs\"\"\"\n",
    "        words = text.lower().split()\n",
    "        token_ids = []\n",
    "        \n",
    "        for word in words:\n",
    "            if word not in self.word_to_id:\n",
    "                self.word_to_id[word] = self.next_id\n",
    "                self.id_to_word[self.next_id] = word\n",
    "                self.next_id += 1\n",
    "            token_ids.append(self.word_to_id[word])\n",
    "        \n",
    "        return token_ids\n",
    "\n",
    "# Create synthetic dataset\n",
    "passages = [\n",
    "    \"The Eiffel Tower is a wrought-iron lattice tower in Paris, France.\",\n",
    "    \"The Great Wall of China is a series of fortifications in northern China.\",\n",
    "    \"The Statue of Liberty is a colossal neoclassical sculpture in New York.\",\n",
    "    \"The Colosseum is an oval amphitheatre in the centre of Rome, Italy.\",\n",
    "    \"The Taj Mahal is an ivory-white marble mausoleum in Agra, India.\",\n",
    "    \"Mount Everest is Earth's highest mountain above sea level.\",\n",
    "    \"The Amazon River is the largest river by discharge volume of water.\",\n",
    "    \"The Sahara is a desert on the African continent.\",\n",
    "]\n",
    "\n",
    "questions = [\n",
    "    (\"What is the Eiffel Tower?\", 0),  # (question, relevant_passage_idx)\n",
    "    (\"Where is the Great Wall located?\", 1),\n",
    "    (\"What is the tallest mountain?\", 5),\n",
    "    (\"Where is the Statue of Liberty?\", 2),\n",
    "    (\"What is the largest river?\", 6),\n",
    "]\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = SimpleTokenizer()\n",
    "\n",
    "passage_tokens = [tokenizer.tokenize(p) for p in passages]\n",
    "question_tokens = [(tokenizer.tokenize(q), idx) for q, idx in questions]\n",
    "\n",
    "print(\"Sample passage:\")\n",
    "print(f\"Text: {passages[0]}\")\n",
    "print(f\"Tokens: {passage_tokens[0][:10]}...\")\n",
    "print(f\"\\nVocabulary size: {tokenizer.next_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Corpus and Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize encoders with correct vocab size\n",
    "vocab_size = tokenizer.next_id\n",
    "question_encoder = SimpleTextEncoder(vocab_size, embedding_dim=32, hidden_dim=64)\n",
    "passage_encoder = SimpleTextEncoder(vocab_size, embedding_dim=32, hidden_dim=64)\n",
    "\n",
    "# Encode all passages\n",
    "passage_embeddings = []\n",
    "for tokens in passage_tokens:\n",
    "    emb = passage_encoder.encode(tokens)\n",
    "    passage_embeddings.append(emb)\n",
    "passage_embeddings = np.array(passage_embeddings)\n",
    "\n",
    "# Encode questions\n",
    "question_embeddings = []\n",
    "for tokens, _ in question_tokens:\n",
    "    emb = question_encoder.encode(tokens)\n",
    "    question_embeddings.append(emb)\n",
    "question_embeddings = np.array(question_embeddings)\n",
    "\n",
    "print(f\"Passage embeddings: {passage_embeddings.shape}\")\n",
    "print(f\"Question embeddings: {question_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Retrieval via Maximum Inner Product Search (MIPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_k(query_embedding, passage_embeddings, k=3):\n",
    "    \"\"\"\n",
    "    Retrieve top-k passages for query\n",
    "    Uses dot product similarity (MIPS)\n",
    "    \"\"\"\n",
    "    # Compute similarities\n",
    "    similarities = np.dot(passage_embeddings, query_embedding)\n",
    "    \n",
    "    # Get top-k indices\n",
    "    top_k_indices = np.argsort(similarities)[::-1][:k]\n",
    "    top_k_scores = similarities[top_k_indices]\n",
    "    \n",
    "    return top_k_indices, top_k_scores\n",
    "\n",
    "# Test retrieval\n",
    "print(\"\\nDense Retrieval Results:\\n\" + \"=\"*80)\n",
    "for i, (q_tokens, correct_idx) in enumerate(question_tokens):\n",
    "    question_text = questions[i][0]\n",
    "    q_emb = question_embeddings[i]\n",
    "    \n",
    "    # Retrieve\n",
    "    top_indices, top_scores = retrieve_top_k(q_emb, passage_embeddings, k=3)\n",
    "    \n",
    "    print(f\"\\nQ: {question_text}\")\n",
    "    print(f\"Correct passage: #{correct_idx}\")\n",
    "    print(f\"\\nRetrieved (top-3):\")\n",
    "    for rank, (idx, score) in enumerate(zip(top_indices, top_scores), 1):\n",
    "        is_correct = \"✓\" if idx == correct_idx else \"✗\"\n",
    "        print(f\"  {rank}. [{is_correct}] (score={score:.3f}) {passages[idx][:60]}...\")\n",
    "\nprint(\"\\n\" + \"=\"*80)\n",
    "print(\"(Encoders are untrained, so results are random)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with In-Batch Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # Numerical stability\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "def contrastive_loss(query_emb, positive_emb, negative_embs):\n",
    "    \"\"\"\n",
    "    Contrastive loss (InfoNCE)\n",
    "    \n",
    "    L = -log( exp(q·p+) / (exp(q·p+) + Σ exp(q·p-)) )\n",
    "    \"\"\"\n",
    "    # Positive score\n",
    "    pos_score = np.dot(query_emb, positive_emb)\n",
    "    \n",
    "    # Negative scores\n",
    "    neg_scores = [np.dot(query_emb, neg_emb) for neg_emb in negative_embs]\n",
    "    \n",
    "    # All scores\n",
    "    all_scores = np.array([pos_score] + neg_scores)\n",
    "    \n",
    "    # Softmax\n",
    "    probs = softmax(all_scores)\n",
    "    \n",
    "    # Negative log likelihood (positive should be first)\n",
    "    loss = -np.log(probs[0] + 1e-8)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "# Simulate training batch\n",
    "batch_size = 3\n",
    "batch_questions = question_embeddings[:batch_size]\n",
    "batch_passages = passage_embeddings[:batch_size]\n",
    "\n",
    "# In-batch negatives: for each question, other passages in batch are negatives\n",
    "total_loss = 0\n",
    "print(\"\\nIn-Batch Negative Training:\\n\" + \"=\"*80)\n",
    "for i in range(batch_size):\n",
    "    q_emb = batch_questions[i]\n",
    "    pos_emb = batch_passages[i]  # Correct passage\n",
    "    \n",
    "    # Negatives: all other passages in batch\n",
    "    neg_embs = [batch_passages[j] for j in range(batch_size) if j != i]\n",
    "    \n",
    "    loss = contrastive_loss(q_emb, pos_emb, neg_embs)\n",
    "    total_loss += loss\n",
    "    \n",
    "    print(f\"Question {i}: loss = {loss:.4f}\")\n",
    "\n",
    "avg_loss = total_loss / batch_size\n",
    "print(f\"\\nAverage batch loss: {avg_loss:.4f}\")\n",
    "print(\"\\nIn-batch negatives: efficient hard negative mining!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Embedding Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple 2D projection (PCA-like)\n",
    "def project_2d(embeddings):\n",
    "    \"\"\"Project high-dim embeddings to 2D (simplified PCA)\"\"\"\n",
    "    # Mean center\n",
    "    mean = np.mean(embeddings, axis=0)\n",
    "    centered = embeddings - mean\n",
    "    \n",
    "    # Take first 2 principal components (simplified)\n",
    "    U, S, Vt = np.linalg.svd(centered, full_matrices=False)\n",
    "    projected = U[:, :2] * S[:2]\n",
    "    \n",
    "    return projected\n",
    "\n",
    "# Project to 2D\n",
    "all_embeddings = np.vstack([passage_embeddings, question_embeddings])\n",
    "projected = project_2d(all_embeddings)\n",
    "\n",
    "passage_2d = projected[:len(passage_embeddings)]\n",
    "question_2d = projected[len(passage_embeddings):]\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Plot passages\n",
    "plt.scatter(passage_2d[:, 0], passage_2d[:, 1], s=200, c='lightblue', \n",
    "           edgecolors='black', linewidths=2, marker='s', label='Passages', zorder=2)\n",
    "\n",
    "# Annotate passages\n",
    "for i, (x, y) in enumerate(passage_2d):\n",
    "    plt.text(x, y-0.15, f'P{i}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Plot questions\n",
    "plt.scatter(question_2d[:, 0], question_2d[:, 1], s=200, c='lightcoral', \n",
    "           edgecolors='black', linewidths=2, marker='o', label='Questions', zorder=3)\n",
    "\n",
    "# Annotate questions\n",
    "for i, (x, y) in enumerate(question_2d):\n",
    "    plt.text(x, y+0.15, f'Q{i}', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Draw connections (question to correct passage)\n",
    "for i, (q_tokens, correct_idx) in enumerate(question_tokens):\n",
    "    q_pos = question_2d[i]\n",
    "    p_pos = passage_2d[correct_idx]\n",
    "    plt.plot([q_pos[0], p_pos[0]], [q_pos[1], p_pos[1]], \n",
    "            'g--', alpha=0.5, linewidth=2, label='Correct' if i == 0 else '')\n",
    "\n",
    "plt.xlabel('Dimension 1', fontsize=12)\n",
    "plt.ylabel('Dimension 2', fontsize=12)\n",
    "plt.title('Dense Retrieval Embedding Space (2D Projection)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nIdeal: Questions close to their relevant passages!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with BM25 (Sparse Retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBM25:\n",
    "    \"\"\"Simplified BM25 scoring\"\"\"\n",
    "    def __init__(self, passages, k1=1.5, b=0.75):\n",
    "        self.passages = passages\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        \n",
    "        # Compute document frequencies\n",
    "        self.doc_freqs = {}\n",
    "        self.avg_doc_len = 0\n",
    "        \n",
    "        all_words = []\n",
    "        for passage in passages:\n",
    "            words = set(passage.lower().split())\n",
    "            all_words.extend(passage.lower().split())\n",
    "            for word in words:\n",
    "                self.doc_freqs[word] = self.doc_freqs.get(word, 0) + 1\n",
    "        \n",
    "        self.avg_doc_len = len(all_words) / len(passages)\n",
    "        self.N = len(passages)\n",
    "    \n",
    "    def score(self, query, passage_idx):\n",
    "        \"\"\"BM25 score for query and passage\"\"\"\n",
    "        query_words = query.lower().split()\n",
    "        passage = self.passages[passage_idx]\n",
    "        passage_words = passage.lower().split()\n",
    "        passage_len = len(passage_words)\n",
    "        \n",
    "        # Count term frequencies\n",
    "        tf = Counter(passage_words)\n",
    "        \n",
    "        score = 0\n",
    "        for word in query_words:\n",
    "            if word not in tf:\n",
    "                continue\n",
    "            \n",
    "            # IDF\n",
    "            df = self.doc_freqs.get(word, 0)\n",
    "            idf = np.log((self.N - df + 0.5) / (df + 0.5) + 1)\n",
    "            \n",
    "            # TF component\n",
    "            freq = tf[word]\n",
    "            norm = 1 - self.b + self.b * (passage_len / self.avg_doc_len)\n",
    "            tf_component = (freq * (self.k1 + 1)) / (freq + self.k1 * norm)\n",
    "            \n",
    "            score += idf * tf_component\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    def retrieve(self, query, k=3):\n",
    "        \"\"\"Retrieve top-k passages for query\"\"\"\n",
    "        scores = [self.score(query, i) for i in range(len(self.passages))]\n",
    "        top_k_indices = np.argsort(scores)[::-1][:k]\n",
    "        top_k_scores = [scores[i] for i in top_k_indices]\n",
    "        return top_k_indices, top_k_scores\n",
    "\n",
    "# Create BM25 retriever\n",
    "bm25 = SimpleBM25(passages)\n",
    "\n",
    "# Compare BM25 vs Dense\n",
    "print(\"\\nBM25 vs Dense Retrieval Comparison:\\n\" + \"=\"*80)\n",
    "for i, (question_text, correct_idx) in enumerate(questions):\n",
    "    print(f\"\\nQ: {question_text}\")\n",
    "    print(f\"Correct: #{correct_idx}\")\n",
    "    \n",
    "    # BM25\n",
    "    bm25_indices, bm25_scores = bm25.retrieve(question_text, k=3)\n",
    "    print(f\"\\nBM25 Top-3:\")\n",
    "    for rank, (idx, score) in enumerate(zip(bm25_indices, bm25_scores), 1):\n",
    "        is_correct = \"✓\" if idx == correct_idx else \"✗\"\n",
    "        print(f\"  {rank}. [{is_correct}] (score={score:.3f}) #{idx}\")\n",
    "    \n",
    "    # Dense\n",
    "    q_emb = question_embeddings[i]\n",
    "    dense_indices, dense_scores = retrieve_top_k(q_emb, passage_embeddings, k=3)\n",
    "    print(f\"\\nDense Top-3:\")\n",
    "    for rank, (idx, score) in enumerate(zip(dense_indices, dense_scores), 1):\n",
    "        is_correct = \"✓\" if idx == correct_idx else \"✗\"\n",
    "        print(f\"  {rank}. [{is_correct}] (score={score:.3f}) #{idx}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BM25: Lexical matching (sparse)\")\n",
    "print(\"Dense: Semantic matching (dense embeddings)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(predictions, correct_indices, k_values=[1, 3, 5]):\n",
    "    \"\"\"\n",
    "    Compute retrieval metrics:\n",
    "    - Recall@k: % of queries where correct passage is in top-k\n",
    "    - MRR (Mean Reciprocal Rank): average 1/rank of correct passage\n",
    "    \"\"\"\n",
    "    n_queries = len(predictions)\n",
    "    \n",
    "    recalls = {k: 0 for k in k_values}\n",
    "    reciprocal_ranks = []\n",
    "    \n",
    "    for pred, correct_idx in zip(predictions, correct_indices):\n",
    "        # Find rank of correct passage\n",
    "        if correct_idx in pred:\n",
    "            rank = list(pred).index(correct_idx) + 1\n",
    "            reciprocal_ranks.append(1.0 / rank)\n",
    "            \n",
    "            # Update recall@k\n",
    "            for k in k_values:\n",
    "                if rank <= k:\n",
    "                    recalls[k] += 1\n",
    "        else:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "    \n",
    "    # Compute averages\n",
    "    mrr = np.mean(reciprocal_ranks)\n",
    "    recalls = {k: v / n_queries for k, v in recalls.items()}\n",
    "    \n",
    "    return recalls, mrr\n",
    "\n",
    "# Evaluate both methods\n",
    "bm25_predictions = []\n",
    "dense_predictions = []\n",
    "correct_indices = []\n",
    "\n",
    "for i, (question_text, correct_idx) in enumerate(questions):\n",
    "    # BM25\n",
    "    bm25_top, _ = bm25.retrieve(question_text, k=5)\n",
    "    bm25_predictions.append(bm25_top)\n",
    "    \n",
    "    # Dense\n",
    "    q_emb = question_embeddings[i]\n",
    "    dense_top, _ = retrieve_top_k(q_emb, passage_embeddings, k=5)\n",
    "    dense_predictions.append(dense_top)\n",
    "    \n",
    "    correct_indices.append(correct_idx)\n",
    "\n",
    "# Compute metrics\n",
    "bm25_recalls, bm25_mrr = compute_metrics(bm25_predictions, correct_indices)\n",
    "dense_recalls, dense_mrr = compute_metrics(dense_predictions, correct_indices)\n",
    "\n",
    "# Display\n",
    "print(\"\\nRetrieval Metrics:\\n\" + \"=\"*60)\n",
    "print(f\"{'Metric':<15} {'BM25':<15} {'Dense':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for k in [1, 3, 5]:\n",
    "    print(f\"Recall@{k:<10} {bm25_recalls[k]:<15.2%} {dense_recalls[k]:<15.2%}\")\n",
    "print(f\"MRR{'':<12} {bm25_mrr:<15.3f} {dense_mrr:<15.3f}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n(Models are untrained - results are random)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Dense Passage Retrieval (DPR) Architecture:\n",
    "\n",
    "**Dual Encoder**:\n",
    "```\n",
    "Question: q → BERT_Q → E_Q(q) = q_emb\n",
    "Passage:  p → BERT_P → E_P(p) = p_emb\n",
    "\n",
    "Similarity: sim(q, p) = q_emb · p_emb\n",
    "```\n",
    "\n",
    "### Training Objective:\n",
    "\n",
    "**Contrastive Loss (InfoNCE)**:\n",
    "$$\n",
    "L(q_i, p_i^+, p_i^{-1}, ..., p_i^{-n}) = -\\log \\frac{e^{\\text{sim}(q_i, p_i^+)}}{e^{\\text{sim}(q_i, p_i^+)} + \\sum_j e^{\\text{sim}(q_i, p_i^{-j})}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $p_i^+$: Positive (relevant) passage\n",
    "- $p_i^{-j}$: Negative (irrelevant) passages\n",
    "\n",
    "### In-Batch Negatives:\n",
    "\n",
    "Efficient negative mining:\n",
    "```\n",
    "Batch: [(q1, p1+), (q2, p2+), ..., (qB, pB+)]\n",
    "\n",
    "For q1:\n",
    "  Positive: p1+\n",
    "  Negatives: p2+, p3+, ..., pB+ (from other examples)\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- No extra passages needed\n",
    "- Gradient flows through all examples\n",
    "- Scales to large batch sizes\n",
    "\n",
    "### Hard Negative Mining:\n",
    "\n",
    "1. **BM25 negatives**: Top BM25 results that aren't relevant\n",
    "2. **Random negatives**: Random passages from corpus\n",
    "3. **In-batch negatives**: Other positives in batch\n",
    "\n",
    "**Best**: Combine all three!\n",
    "\n",
    "### Inference (Retrieval):\n",
    "\n",
    "**Offline**:\n",
    "1. Encode all passages: $P = \\{E_P(p_1), ..., E_P(p_N)\\}$\n",
    "2. Build MIPS index (e.g., FAISS)\n",
    "\n",
    "**Online** (at query time):\n",
    "1. Encode query: $q_{emb} = E_Q(q)$\n",
    "2. Search index: top-k by $\\arg\\max_p \\, q_{emb} \\cdot p_{emb}$\n",
    "\n",
    "### DPR vs BM25:\n",
    "\n",
    "| Aspect | BM25 | DPR |\n",
    "|--------|------|-----|\n",
    "| Matching | Lexical (exact words) | Semantic (meaning) |\n",
    "| Training | None (heuristic) | Learned from data |\n",
    "| Robustness | Sensitive to wording | Handles paraphrases |\n",
    "| Speed | Fast (sparse) | Fast with MIPS index |\n",
    "| Memory | Low | High (dense vectors) |\n",
    "\n",
    "### Results (from paper):\n",
    "\n",
    "**Natural Questions**:\n",
    "- BM25: 59.1% Top-20 accuracy\n",
    "- DPR: 78.4% Top-20 accuracy\n",
    "\n",
    "**WebQuestions**:\n",
    "- BM25: 55.0%\n",
    "- DPR: 75.0%\n",
    "\n",
    "**TREC**:\n",
    "- BM25: 70.9%\n",
    "- DPR: 79.4%\n",
    "\n",
    "### Implementation Details:\n",
    "\n",
    "1. **Encoders**: BERT-base (110M params)\n",
    "2. **Embedding dim**: 768 (BERT hidden size)\n",
    "3. **Batch size**: 128 (large for in-batch negatives)\n",
    "4. **Hard negatives**: 1 BM25 + 1 random per positive\n",
    "5. **Training**: ~40 epochs on 59k QA pairs\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "- ✅ **Semantic matching**: Understands meaning, not just words\n",
    "- ✅ **End-to-end**: Learned from question-passage pairs\n",
    "- ✅ **Handles paraphrases**: \"tallest mountain\" = \"highest peak\"\n",
    "- ✅ **Scalable**: MIPS with FAISS for billions of passages\n",
    "- ✅ **Outperforms BM25**: +15-20% absolute accuracy\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "- ❌ **Requires training data**: Need QA pairs\n",
    "- ❌ **Memory**: Dense vectors for all passages\n",
    "- ❌ **Index updates**: Re-encode when corpus changes\n",
    "- ❌ **May miss exact matches**: BM25 better for rare entities\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Hybrid retrieval**: Combine BM25 + DPR\n",
    "2. **Large batches**: More in-batch negatives\n",
    "3. **Hard negatives**: Use BM25 top results\n",
    "4. **Fine-tune**: Domain-specific data improves results\n",
    "5. **FAISS**: Use for fast MIPS at scale\n",
    "\n",
    "### Modern Extensions:\n",
    "\n",
    "- **ColBERT**: Late interaction for better ranking\n",
    "- **ANCE**: Approximate nearest neighbor negatives\n",
    "- **RocketQA**: Cross-batch negatives\n",
    "- **Contriever**: Unsupervised dense retrieval\n",
    "- **Dense X Retrieval**: Multi-vector representations\n",
    "\n",
    "### Applications:\n",
    "\n",
    "- Open-domain QA (e.g., Google search)\n",
    "- RAG (Retrieval-Augmented Generation)\n",
    "- Document search\n",
    "- Semantic search\n",
    "- Knowledge base completion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
