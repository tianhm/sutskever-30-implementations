{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper 26: CS231n - Convolutional Neural Networks for Visual Recognition\n",
    "\n",
    "**Course**: Stanford CS231n (Spring 2017) - Fei-Fei Li, Justin Johnson, Serena Yeung\n",
    "\n",
    "**The Vision Bible**: CS231n is the definitive course on deep learning for computer vision. This notebook distills its core concepts into a single, executable implementation using only NumPy.\n",
    "\n",
    "---\n",
    "\n",
    "## What is CS231n?\n",
    "\n",
    "CS231n teaches the foundations of visual recognition:\n",
    "- Image classification pipeline (from pixels to predictions)\n",
    "- Backpropagation and optimization\n",
    "- Convolutional neural networks\n",
    "- Modern architectures (AlexNet, VGG, ResNet)\n",
    "- Training techniques and \"babysitting\" neural nets\n",
    "\n",
    "## This Implementation\n",
    "\n",
    "We'll build the complete vision pipeline from scratch:\n",
    "\n",
    "1. **k-Nearest Neighbors**: Baseline classifier\n",
    "2. **Linear Classifiers**: SVM and Softmax\n",
    "3. **Optimization**: SGD, momentum, learning rate schedules\n",
    "4. **Neural Networks**: 2-layer fully-connected networks\n",
    "5. **Backpropagation**: Manual gradient computation\n",
    "6. **Convolutional Networks**: Conv, pool, ReLU layers\n",
    "7. **Architectures**: AlexNet-style CNNs, VGG, ResNet concepts\n",
    "8. **Visualization**: Saliency maps, filter visualization\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "**CS231n principles apply everywhere**:\n",
    "- AlexNet (2012) â†’ ImageNet breakthrough\n",
    "- VGG/ResNet â†’ Standard vision backbones\n",
    "- Techniques here â†’ Modern transformers, diffusion models\n",
    "\n",
    "**Connection to Paper #7**: This provides the pedagogical foundation for AlexNet!\n",
    "\n",
    "Let's build vision systems from first principles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import convolve\n",
    "from typing import Tuple, List, Dict\n",
    "from dataclasses import dataclass\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"CS231n: From Pixels to Predictions\")\n",
    "print(\"NumPy version:\", np.__version__)\n",
    "print(\"\\nReady to learn computer vision!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Dataset - Synthetic CIFAR-10\n",
    "\n",
    "CS231n uses CIFAR-10 (10 classes, 32Ã—32 RGB images). We'll generate synthetic data with similar structure.\n",
    "\n",
    "## Data Generation Strategy\n",
    "\n",
    "Create procedural 32Ã—32 images with class-specific patterns:\n",
    "- **Class 0-2**: Spirals (different rotations)\n",
    "- **Class 3-5**: Checkerboards (different frequencies)\n",
    "- **Class 6-7**: Gradients (different directions)\n",
    "- **Class 8-9**: Circles (different sizes)\n",
    "\n",
    "This gives us:\n",
    "- Learnable patterns (not pure noise)\n",
    "- Visual diversity (test different features)\n",
    "- Instant generation (no downloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_cifar(num_samples: int = 1000, \n",
    "                             img_size: int = 32, \n",
    "                             num_classes: int = 10) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Generate synthetic CIFAR-10 style dataset.\n",
    "    \n",
    "    Returns:\n",
    "        X: (N, 32, 32, 3) RGB images\n",
    "        y: (N,) class labels\n",
    "    \"\"\"\n",
    "    X = np.zeros((num_samples, img_size, img_size, 3))\n",
    "    y = np.random.randint(0, num_classes, num_samples)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        label = y[i]\n",
    "        img = np.random.randn(img_size, img_size, 3) * 0.1  # Base noise\n",
    "        \n",
    "        # Class-specific patterns\n",
    "        if label < 3:  # Spirals\n",
    "            theta = np.linspace(0, 4*np.pi, 200)\n",
    "            r = np.linspace(0, img_size/2, 200)\n",
    "            rotation = label * np.pi / 3\n",
    "            x_coords = (r * np.cos(theta + rotation) + img_size/2).astype(int)\n",
    "            y_coords = (r * np.sin(theta + rotation) + img_size/2).astype(int)\n",
    "            valid = (x_coords >= 0) & (x_coords < img_size) & (y_coords >= 0) & (y_coords < img_size)\n",
    "            img[y_coords[valid], x_coords[valid], :] = [1.0, 0.5, 0.0]\n",
    "            \n",
    "        elif label < 6:  # Checkerboards\n",
    "            freq = (label - 2) * 2\n",
    "            xx, yy = np.meshgrid(np.arange(img_size), np.arange(img_size))\n",
    "            pattern = ((xx // freq) + (yy // freq)) % 2\n",
    "            img[:, :, 0] = pattern\n",
    "            img[:, :, 1] = 1 - pattern\n",
    "            \n",
    "        elif label < 8:  # Gradients\n",
    "            if label == 6:\n",
    "                img[:, :, 0] = np.linspace(0, 1, img_size)[None, :]\n",
    "            else:\n",
    "                img[:, :, 1] = np.linspace(0, 1, img_size)[:, None]\n",
    "                \n",
    "        else:  # Circles\n",
    "            radius = (label - 7) * 8 + 5\n",
    "            yy, xx = np.ogrid[:img_size, :img_size]\n",
    "            circle = ((xx - img_size/2)**2 + (yy - img_size/2)**2 <= radius**2)\n",
    "            img[circle, 2] = 1.0\n",
    "        \n",
    "        X[i] = np.clip(img, 0, 1)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Generate train/val/test splits\n",
    "print(\"Generating synthetic CIFAR-10...\\n\")\n",
    "\n",
    "X_train, y_train = generate_synthetic_cifar(num_samples=2000)\n",
    "X_val, y_val = generate_synthetic_cifar(num_samples=400)\n",
    "X_test, y_test = generate_synthetic_cifar(num_samples=400)\n",
    "\n",
    "print(f\"Training set:   X={X_train.shape}, y={y_train.shape}\")\n",
    "print(f\"Validation set: X={X_val.shape}, y={y_val.shape}\")\n",
    "print(f\"Test set:       X={X_test.shape}, y={y_test.shape}\")\n",
    "\n",
    "# Visualize samples\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(10):\n",
    "    ax = axes[i // 5, i % 5]\n",
    "    idx = np.where(y_train == i)[0][0]\n",
    "    ax.imshow(X_train[idx])\n",
    "    ax.set_title(f'Class {i}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Synthetic CIFAR-10: Sample Images per Class', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Flatten for traditional classifiers\n",
    "X_train_flat = X_train.reshape(len(X_train), -1)  # (N, 3072)\n",
    "X_val_flat = X_val.reshape(len(X_val), -1)\n",
    "X_test_flat = X_test.reshape(len(X_test), -1)\n",
    "\n",
    "print(f\"\\nFlattened shape: {X_train_flat.shape} (32Ã—32Ã—3 = 3072 pixels)\")\n",
    "print(\"\\nâœ“ Dataset ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: k-Nearest Neighbors (kNN)\n",
    "\n",
    "**The simplest classifier**: Given test image, find k closest training images and vote on label.\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "1. Compute distance to all training images: $d(x_{\\text{test}}, x_{\\text{train}})$\n",
    "2. Find k nearest neighbors\n",
    "3. Majority vote on their labels\n",
    "\n",
    "## Distance Metrics\n",
    "\n",
    "**L1 (Manhattan)**:\n",
    "$$d_1(x, y) = \\sum_i |x_i - y_i|$$\n",
    "\n",
    "**L2 (Euclidean)**:\n",
    "$$d_2(x, y) = \\sqrt{\\sum_i (x_i - y_i)^2}$$\n",
    "\n",
    "## Why kNN Matters\n",
    "\n",
    "- **No training**: Just memorize data\n",
    "- **Test-time slow**: O(N) per prediction\n",
    "- **Baseline**: Establishes lower bound\n",
    "- **Never used in practice**: But important pedagogically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbor:\n",
    "    \"\"\"k-Nearest Neighbor classifier.\"\"\"\n",
    "    \n",
    "    def __init__(self, k: int = 5):\n",
    "        self.k = k\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "    \n",
    "    def train(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"'Train' by memorizing data (no actual training!).\"\"\"\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "        print(f\"kNN 'trained' on {len(X)} samples\")\n",
    "    \n",
    "    def predict(self, X: np.ndarray, distance_metric: str = 'l2') -> np.ndarray:\n",
    "        \"\"\"Predict labels for test data.\n",
    "        \n",
    "        Args:\n",
    "            X: (N_test, D) test data\n",
    "            distance_metric: 'l1' or 'l2'\n",
    "        \n",
    "        Returns:\n",
    "            y_pred: (N_test,) predicted labels\n",
    "        \"\"\"\n",
    "        num_test = X.shape[0]\n",
    "        y_pred = np.zeros(num_test, dtype=int)\n",
    "        \n",
    "        for i in range(num_test):\n",
    "            # Compute distances to all training samples\n",
    "            if distance_metric == 'l1':\n",
    "                distances = np.sum(np.abs(self.X_train - X[i]), axis=1)\n",
    "            else:  # l2\n",
    "                distances = np.sqrt(np.sum((self.X_train - X[i])**2, axis=1))\n",
    "            \n",
    "            # Find k nearest neighbors\n",
    "            k_nearest = np.argsort(distances)[:self.k]\n",
    "            k_nearest_labels = self.y_train[k_nearest]\n",
    "            \n",
    "            # Majority vote\n",
    "            y_pred[i] = np.argmax(np.bincount(k_nearest_labels))\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def compute_accuracy(self, X: np.ndarray, y: np.ndarray, **kwargs) -> float:\n",
    "        \"\"\"Compute classification accuracy.\"\"\"\n",
    "        y_pred = self.predict(X, **kwargs)\n",
    "        return np.mean(y_pred == y)\n",
    "\n",
    "\n",
    "# Train kNN (just memorize)\n",
    "print(\"Testing k-Nearest Neighbors...\\n\")\n",
    "\n",
    "knn = KNearestNeighbor(k=5)\n",
    "knn.train(X_train_flat, y_train)\n",
    "\n",
    "# Test different k values\n",
    "k_values = [1, 3, 5, 10, 20]\n",
    "accuracies_l1 = []\n",
    "accuracies_l2 = []\n",
    "\n",
    "print(\"\\nTesting different k values...\")\n",
    "for k in k_values:\n",
    "    knn.k = k\n",
    "    acc_l1 = knn.compute_accuracy(X_val_flat[:100], y_val[:100], distance_metric='l1')\n",
    "    acc_l2 = knn.compute_accuracy(X_val_flat[:100], y_val[:100], distance_metric='l2')\n",
    "    accuracies_l1.append(acc_l1)\n",
    "    accuracies_l2.append(acc_l2)\n",
    "    print(f\"  k={k:2d}: L1={acc_l1:.1%}, L2={acc_l2:.1%}\")\n",
    "\n",
    "# Plot accuracy vs k\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Accuracy vs k\n",
    "axes[0].plot(k_values, accuracies_l1, 'o-', linewidth=2, markersize=8, label='L1 distance')\n",
    "axes[0].plot(k_values, accuracies_l2, 's-', linewidth=2, markersize=8, label='L2 distance')\n",
    "axes[0].set_xlabel('k (number of neighbors)', fontsize=11)\n",
    "axes[0].set_ylabel('Validation Accuracy', fontsize=11)\n",
    "axes[0].set_title('kNN: Hyperparameter Tuning', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Confusion matrix for k=5\n",
    "knn.k = 5\n",
    "y_pred = knn.predict(X_val_flat[:200], distance_metric='l2')\n",
    "y_true = y_val[:200]\n",
    "\n",
    "confusion = np.zeros((10, 10))\n",
    "for true, pred in zip(y_true, y_pred):\n",
    "    confusion[true, pred] += 1\n",
    "\n",
    "im = axes[1].imshow(confusion, cmap='Blues')\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=11)\n",
    "axes[1].set_ylabel('True Label', fontsize=11)\n",
    "axes[1].set_title('Confusion Matrix (k=5, L2)', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ”‘ Key insights:\")\n",
    "print(\"   â€¢ kNN: No training, slow at test time\")\n",
    "print(\"   â€¢ k=1: Overfits (memorizes noise)\")\n",
    "print(\"   â€¢ k too large: Underfits (averages too much)\")\n",
    "print(\"   â€¢ Best k: Found via validation set\")\n",
    "print(f\"   â€¢ Best accuracy: {max(max(accuracies_l1), max(accuracies_l2)):.1%} (baseline!)\")\n",
    "print(\"\\nâœ“ kNN complete! Let's do better with parametric models...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3: Linear Classifiers - SVM and Softmax\n",
    "\n",
    "**Parametric models**: Learn weight matrix $W$ to predict scores.\n",
    "\n",
    "## Score Function\n",
    "\n",
    "$$f(x; W, b) = Wx + b$$\n",
    "\n",
    "where:\n",
    "- $x \\in \\mathbb{R}^D$: Input image (3072 pixels)\n",
    "- $W \\in \\mathbb{R}^{C \\times D}$: Weight matrix (10 Ã— 3072)\n",
    "- $b \\in \\mathbb{R}^C$: Bias vector (10,)\n",
    "- Output: $f \\in \\mathbb{R}^C$: Class scores (10,)\n",
    "\n",
    "## Loss Functions\n",
    "\n",
    "### 1. Multiclass SVM Loss (Hinge Loss)\n",
    "\n",
    "$$L = \\frac{1}{N} \\sum_{i=1}^N \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i} + \\Delta)$$\n",
    "\n",
    "where $\\Delta = 1$ is the margin.\n",
    "\n",
    "**Intuition**: Correct class score should be at least $\\Delta$ higher than wrong classes.\n",
    "\n",
    "### 2. Softmax Loss (Cross-Entropy)\n",
    "\n",
    "$$L = -\\frac{1}{N} \\sum_{i=1}^N \\log\\left(\\frac{e^{s_{y_i}}}{\\sum_j e^{s_j}}\\right)$$\n",
    "\n",
    "**Intuition**: Maximize log-probability of correct class.\n",
    "\n",
    "## Regularization\n",
    "\n",
    "Add penalty to prevent overfitting:\n",
    "\n",
    "$$L_{\\text{total}} = L_{\\text{data}} + \\lambda R(W)$$\n",
    "\n",
    "Common choices:\n",
    "- **L2**: $R(W) = \\sum_{i,j} W_{ij}^2$ (weight decay)\n",
    "- **L1**: $R(W) = \\sum_{i,j} |W_{ij}|$ (sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearClassifier:\n",
    "    \"\"\"Linear classifier with SVM or Softmax loss.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 3072, num_classes: int = 10):\n",
    "        self.W = np.random.randn(input_dim, num_classes) * 0.0001\n",
    "        self.b = np.zeros(num_classes)\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Compute class scores.\n",
    "        \n",
    "        Args:\n",
    "            X: (N, D) input data\n",
    "        \n",
    "        Returns:\n",
    "            scores: (N, C) class scores\n",
    "        \"\"\"\n",
    "        return X @ self.W + self.b\n",
    "    \n",
    "    def svm_loss(self, X: np.ndarray, y: np.ndarray, reg: float = 1e-5) -> Tuple[float, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Compute SVM loss and gradients.\n",
    "        \n",
    "        Returns:\n",
    "            loss: Scalar loss\n",
    "            dW: Gradient of loss w.r.t. W\n",
    "            db: Gradient of loss w.r.t. b\n",
    "        \"\"\"\n",
    "        N = X.shape[0]\n",
    "        scores = self.forward(X)  # (N, C)\n",
    "        \n",
    "        # Compute margins\n",
    "        correct_scores = scores[range(N), y].reshape(-1, 1)  # (N, 1)\n",
    "        margins = np.maximum(0, scores - correct_scores + 1)  # (N, C)\n",
    "        margins[range(N), y] = 0  # Don't count correct class\n",
    "        \n",
    "        # Loss\n",
    "        loss = np.sum(margins) / N\n",
    "        loss += reg * np.sum(self.W ** 2)  # L2 regularization\n",
    "        \n",
    "        # Gradients\n",
    "        binary = (margins > 0).astype(float)  # (N, C)\n",
    "        binary[range(N), y] = -np.sum(binary, axis=1)  # Correct class gets negative\n",
    "        \n",
    "        dW = (X.T @ binary) / N + 2 * reg * self.W\n",
    "        db = np.sum(binary, axis=0) / N\n",
    "        \n",
    "        return loss, dW, db\n",
    "    \n",
    "    def softmax_loss(self, X: np.ndarray, y: np.ndarray, reg: float = 1e-5) -> Tuple[float, np.ndarray, np.ndarray]:\n",
    "        \"\"\"Compute Softmax loss and gradients.\n",
    "        \n",
    "        Returns:\n",
    "            loss: Scalar loss\n",
    "            dW: Gradient of loss w.r.t. W\n",
    "            db: Gradient of loss w.r.t. b\n",
    "        \"\"\"\n",
    "        N = X.shape[0]\n",
    "        scores = self.forward(X)  # (N, C)\n",
    "        \n",
    "        # Numerical stability: shift scores\n",
    "        scores -= np.max(scores, axis=1, keepdims=True)\n",
    "        \n",
    "        # Softmax probabilities\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)  # (N, C)\n",
    "        \n",
    "        # Loss\n",
    "        correct_log_probs = -np.log(probs[range(N), y] + 1e-10)\n",
    "        loss = np.sum(correct_log_probs) / N\n",
    "        loss += reg * np.sum(self.W ** 2)\n",
    "        \n",
    "        # Gradients\n",
    "        dscores = probs.copy()\n",
    "        dscores[range(N), y] -= 1  # Subtract 1 from correct class\n",
    "        dscores /= N\n",
    "        \n",
    "        dW = X.T @ dscores + 2 * reg * self.W\n",
    "        db = np.sum(dscores, axis=0)\n",
    "        \n",
    "        return loss, dW, db\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        scores = self.forward(X)\n",
    "        return np.argmax(scores, axis=1)\n",
    "    \n",
    "    def accuracy(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"Compute classification accuracy.\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "\n",
    "\n",
    "def train_linear_classifier(classifier: LinearClassifier,\n",
    "                           X_train: np.ndarray,\n",
    "                           y_train: np.ndarray,\n",
    "                           X_val: np.ndarray,\n",
    "                           y_val: np.ndarray,\n",
    "                           loss_function: str = 'softmax',\n",
    "                           learning_rate: float = 1e-3,\n",
    "                           reg: float = 1e-5,\n",
    "                           num_iters: int = 1000,\n",
    "                           batch_size: int = 200,\n",
    "                           verbose: bool = True) -> Dict:\n",
    "    \"\"\"Train linear classifier using SGD.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with training history\n",
    "    \"\"\"\n",
    "    N = X_train.shape[0]\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    \n",
    "    for it in range(num_iters):\n",
    "        # Sample mini-batch\n",
    "        batch_indices = np.random.choice(N, batch_size, replace=False)\n",
    "        X_batch = X_train[batch_indices]\n",
    "        y_batch = y_train[batch_indices]\n",
    "        \n",
    "        # Compute loss and gradients\n",
    "        if loss_function == 'svm':\n",
    "            loss, dW, db = classifier.svm_loss(X_batch, y_batch, reg)\n",
    "        else:  # softmax\n",
    "            loss, dW, db = classifier.softmax_loss(X_batch, y_batch, reg)\n",
    "        \n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # Update parameters\n",
    "        classifier.W -= learning_rate * dW\n",
    "        classifier.b -= learning_rate * db\n",
    "        \n",
    "        # Check accuracy periodically\n",
    "        if it % 100 == 0:\n",
    "            train_acc = classifier.accuracy(X_train[:1000], y_train[:1000])\n",
    "            val_acc = classifier.accuracy(X_val, y_val)\n",
    "            train_acc_history.append(train_acc)\n",
    "            val_acc_history.append(val_acc)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Iter {it:4d}/{num_iters}: Loss={loss:.4f}, Train Acc={train_acc:.2%}, Val Acc={val_acc:.2%}\")\n",
    "    \n",
    "    return {\n",
    "        'loss_history': loss_history,\n",
    "        'train_acc_history': train_acc_history,\n",
    "        'val_acc_history': val_acc_history\n",
    "    }\n",
    "\n",
    "\n",
    "# Train Softmax classifier\n",
    "print(\"Training Softmax Classifier...\\n\")\n",
    "\n",
    "softmax_clf = LinearClassifier()\n",
    "softmax_history = train_linear_classifier(\n",
    "    softmax_clf, X_train_flat, y_train, X_val_flat, y_val,\n",
    "    loss_function='softmax',\n",
    "    learning_rate=1e-3,\n",
    "    reg=1e-5,\n",
    "    num_iters=1000\n",
    ")\n",
    "\n",
    "# Train SVM classifier for comparison\n",
    "print(\"\\nTraining SVM Classifier...\\n\")\n",
    "\n",
    "svm_clf = LinearClassifier()\n",
    "svm_history = train_linear_classifier(\n",
    "    svm_clf, X_train_flat, y_train, X_val_flat, y_val,\n",
    "    loss_function='svm',\n",
    "    learning_rate=1e-3,\n",
    "    reg=1e-5,\n",
    "    num_iters=1000\n",
    ")\n",
    "\n",
    "# Visualize training\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Loss curves\n",
    "axes[0].plot(softmax_history['loss_history'], label='Softmax', alpha=0.7)\n",
    "axes[0].plot(svm_history['loss_history'], label='SVM', alpha=0.7)\n",
    "axes[0].set_xlabel('Iteration', fontsize=11)\n",
    "axes[0].set_ylabel('Loss', fontsize=11)\n",
    "axes[0].set_title('Training Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy curves\n",
    "iters_check = np.arange(0, 1000, 100)\n",
    "axes[1].plot(iters_check, softmax_history['val_acc_history'], 'o-', label='Softmax', linewidth=2)\n",
    "axes[1].plot(iters_check, svm_history['val_acc_history'], 's-', label='SVM', linewidth=2)\n",
    "axes[1].set_xlabel('Iteration', fontsize=11)\n",
    "axes[1].set_ylabel('Validation Accuracy', fontsize=11)\n",
    "axes[1].set_title('Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Visualize learned weights (as images)\n",
    "W_img = softmax_clf.W.T.reshape(10, 32, 32, 3)  # (10, 32, 32, 3)\n",
    "W_grid = np.zeros((32*2, 32*5, 3))\n",
    "for i in range(10):\n",
    "    row, col = i // 5, i % 5\n",
    "    W_normalized = (W_img[i] - W_img[i].min()) / (W_img[i].max() - W_img[i].min() + 1e-10)\n",
    "    W_grid[row*32:(row+1)*32, col*32:(col+1)*32] = W_normalized\n",
    "\n",
    "axes[2].imshow(W_grid)\n",
    "axes[2].set_title('Learned Weight Templates', fontsize=12, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final test accuracy\n",
    "test_acc_softmax = softmax_clf.accuracy(X_test_flat, y_test)\n",
    "test_acc_svm = svm_clf.accuracy(X_test_flat, y_test)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"Final Test Accuracy:\")\n",
    "print(f\"  Softmax: {test_acc_softmax:.2%}\")\n",
    "print(f\"  SVM:     {test_acc_svm:.2%}\")\n",
    "print(f\"  kNN:     {max(max(accuracies_l1), max(accuracies_l2)):.2%} (baseline)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nðŸ”‘ Key insights:\")\n",
    "print(\"   â€¢ Linear classifier: f(x) = Wx + b (one template per class)\")\n",
    "print(\"   â€¢ SVM: Margin-based (hinge loss)\")\n",
    "print(\"   â€¢ Softmax: Probability-based (cross-entropy)\")\n",
    "print(\"   â€¢ Both outperform kNN and train fast!\")\n",
    "print(\"   â€¢ Weights look like averaged class templates\")\n",
    "print(\"\\nâœ“ Linear classifiers complete! Let's add nonlinearity...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Optimization - SGD, Momentum, and Learning Rate Schedules\n",
    "\n",
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Update rule:\n",
    "$$w_{t+1} = w_t - \\eta \\nabla L(w_t)$$\n",
    "\n",
    "where $\\eta$ is the learning rate.\n",
    "\n",
    "## SGD with Momentum\n",
    "\n",
    "Add velocity term:\n",
    "$$v_{t+1} = \\rho v_t - \\eta \\nabla L(w_t)$$\n",
    "$$w_{t+1} = w_t + v_{t+1}$$\n",
    "\n",
    "where $\\rho \\in [0, 1]$ is the momentum coefficient (typically 0.9).\n",
    "\n",
    "**Benefit**: Smooths updates, accelerates through ravines.\n",
    "\n",
    "## Learning Rate Schedules\n",
    "\n",
    "**Step decay**:\n",
    "$$\\eta_t = \\eta_0 \\cdot \\gamma^{\\lfloor t / T \\rfloor}$$\n",
    "\n",
    "**Exponential decay**:\n",
    "$$\\eta_t = \\eta_0 e^{-kt}$$\n",
    "\n",
    "**1/t decay**:\n",
    "$$\\eta_t = \\frac{\\eta_0}{1 + kt}$$\n",
    "\n",
    "## Babysitting the Learning Process\n",
    "\n",
    "**CS231n wisdom**:\n",
    "1. Start with small lr (1e-3 to 1e-4)\n",
    "2. Monitor loss: Should decrease smoothly\n",
    "3. Check gradients: Not too small, not too large\n",
    "4. Visualize weights: Should show structure\n",
    "5. Overfit small dataset first (sanity check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"Base optimizer class.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 1e-3):\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def update(self, param: np.ndarray, grad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Update parameter using gradient.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SGD(Optimizer):\n",
    "    \"\"\"Vanilla SGD optimizer.\"\"\"\n",
    "    \n",
    "    def update(self, param: np.ndarray, grad: np.ndarray) -> np.ndarray:\n",
    "        return param - self.learning_rate * grad\n",
    "\n",
    "\n",
    "class SGDMomentum(Optimizer):\n",
    "    \"\"\"SGD with momentum.\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 1e-3, momentum: float = 0.9):\n",
    "        super().__init__(learning_rate)\n",
    "        self.momentum = momentum\n",
    "        self.velocity = {}\n",
    "    \n",
    "    def update(self, param: np.ndarray, grad: np.ndarray, param_id: str = 'default') -> np.ndarray:\n",
    "        if param_id not in self.velocity:\n",
    "            self.velocity[param_id] = np.zeros_like(param)\n",
    "        \n",
    "        self.velocity[param_id] = self.momentum * self.velocity[param_id] - self.learning_rate * grad\n",
    "        return param + self.velocity[param_id]\n",
    "\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    \"\"\"Adam optimizer (adaptive learning rates).\"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate: float = 1e-3, beta1: float = 0.9, beta2: float = 0.999):\n",
    "        super().__init__(learning_rate)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = 1e-8\n",
    "        self.m = {}  # First moment\n",
    "        self.v = {}  # Second moment\n",
    "        self.t = {}  # Timestep\n",
    "    \n",
    "    def update(self, param: np.ndarray, grad: np.ndarray, param_id: str = 'default') -> np.ndarray:\n",
    "        if param_id not in self.m:\n",
    "            self.m[param_id] = np.zeros_like(param)\n",
    "            self.v[param_id] = np.zeros_like(param)\n",
    "            self.t[param_id] = 0\n",
    "        \n",
    "        self.t[param_id] += 1\n",
    "        t = self.t[param_id]\n",
    "        \n",
    "        # Update biased moments\n",
    "        self.m[param_id] = self.beta1 * self.m[param_id] + (1 - self.beta1) * grad\n",
    "        self.v[param_id] = self.beta2 * self.v[param_id] + (1 - self.beta2) * (grad ** 2)\n",
    "        \n",
    "        # Bias correction\n",
    "        m_hat = self.m[param_id] / (1 - self.beta1 ** t)\n",
    "        v_hat = self.v[param_id] / (1 - self.beta2 ** t)\n",
    "        \n",
    "        # Update\n",
    "        return param - self.learning_rate * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "\n",
    "\n",
    "def learning_rate_schedule(initial_lr: float, iteration: int, schedule_type: str = 'step') -> float:\n",
    "    \"\"\"Compute learning rate with schedule.\n",
    "    \n",
    "    Args:\n",
    "        initial_lr: Initial learning rate\n",
    "        iteration: Current iteration\n",
    "        schedule_type: 'step', 'exp', or 'inverse'\n",
    "    \"\"\"\n",
    "    if schedule_type == 'step':\n",
    "        # Decay by 0.5 every 250 iterations\n",
    "        return initial_lr * (0.5 ** (iteration // 250))\n",
    "    elif schedule_type == 'exp':\n",
    "        # Exponential decay\n",
    "        return initial_lr * np.exp(-0.001 * iteration)\n",
    "    else:  # inverse\n",
    "        # 1/t decay\n",
    "        return initial_lr / (1 + 0.001 * iteration)\n",
    "\n",
    "\n",
    "# Compare optimizers\n",
    "print(\"Comparing optimizers...\\n\")\n",
    "\n",
    "optimizers = {\n",
    "    'SGD': SGD(learning_rate=1e-3),\n",
    "    'SGD+Momentum': SGDMomentum(learning_rate=1e-3, momentum=0.9),\n",
    "    'Adam': Adam(learning_rate=1e-3)\n",
    "}\n",
    "\n",
    "histories = {}\n",
    "\n",
    "for name, optimizer in optimizers.items():\n",
    "    print(f\"Training with {name}...\")\n",
    "    clf = LinearClassifier()\n",
    "    \n",
    "    loss_history = []\n",
    "    for it in range(500):\n",
    "        batch_indices = np.random.choice(len(X_train_flat), 200)\n",
    "        X_batch = X_train_flat[batch_indices]\n",
    "        y_batch = y_train[batch_indices]\n",
    "        \n",
    "        loss, dW, db = clf.softmax_loss(X_batch, y_batch, reg=1e-5)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        if isinstance(optimizer, (SGDMomentum, Adam)):\n",
    "            clf.W = optimizer.update(clf.W, dW, 'W')\n",
    "            clf.b = optimizer.update(clf.b, db, 'b')\n",
    "        else:\n",
    "            clf.W = optimizer.update(clf.W, dW)\n",
    "            clf.b = optimizer.update(clf.b, db)\n",
    "    \n",
    "    histories[name] = loss_history\n",
    "    final_acc = clf.accuracy(X_val_flat, y_val)\n",
    "    print(f\"  Final val acc: {final_acc:.2%}\\n\")\n",
    "\n",
    "# Visualize optimizer comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Loss curves\n",
    "for name, history in histories.items():\n",
    "    axes[0].plot(history, label=name, linewidth=2, alpha=0.8)\n",
    "\n",
    "axes[0].set_xlabel('Iteration', fontsize=11)\n",
    "axes[0].set_ylabel('Loss', fontsize=11)\n",
    "axes[0].set_title('Optimizer Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_yscale('log')\n",
    "\n",
    "# Plot 2: Learning rate schedules\n",
    "iters = np.arange(1000)\n",
    "for schedule in ['step', 'exp', 'inverse']:\n",
    "    lrs = [learning_rate_schedule(1e-3, it, schedule) for it in iters]\n",
    "    axes[1].plot(iters, lrs, label=schedule.capitalize(), linewidth=2)\n",
    "\n",
    "axes[1].set_xlabel('Iteration', fontsize=11)\n",
    "axes[1].set_ylabel('Learning Rate', fontsize=11)\n",
    "axes[1].set_title('Learning Rate Schedules', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ”‘ Key insights:\")\n",
    "print(\"   â€¢ SGD: Simple but can be slow\")\n",
    "print(\"   â€¢ Momentum: Smooths updates, accelerates convergence\")\n",
    "print(\"   â€¢ Adam: Adaptive rates, often works out-of-the-box\")\n",
    "print(\"   â€¢ Learning rate schedule: Helps fine-tuning\")\n",
    "print(\"   â€¢ Babysitting: Monitor loss, check gradients, visualize weights\")\n",
    "print(\"\\nâœ“ Optimization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5: Neural Networks - Adding Nonlinearity\n",
    "\n",
    "Linear classifiers have fundamental limits. Neural networks add **nonlinearity** through hidden layers.\n",
    "\n",
    "## 2-Layer Neural Network\n",
    "\n",
    "$$h = \\text{ReLU}(W_1 x + b_1)$$\n",
    "$$y = W_2 h + b_2$$\n",
    "\n",
    "where:\n",
    "- $x \\in \\mathbb{R}^D$: Input (3072)\n",
    "- $W_1 \\in \\mathbb{R}^{D \\times H}$: First layer weights\n",
    "- $h \\in \\mathbb{R}^H$: Hidden layer (e.g., H=100)\n",
    "- $W_2 \\in \\mathbb{R}^{H \\times C}$: Second layer weights\n",
    "- $y \\in \\mathbb{R}^C$: Output scores (10)\n",
    "\n",
    "## Activation Functions\n",
    "\n",
    "**ReLU** (Rectified Linear Unit):\n",
    "$$\\text{ReLU}(x) = \\max(0, x)$$\n",
    "\n",
    "**Sigmoid**:\n",
    "$$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "**Tanh**:\n",
    "$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$\n",
    "\n",
    "**ReLU is preferred**: Fast, no saturation, works well in practice.\n",
    "\n",
    "## Backpropagation\n",
    "\n",
    "Chain rule through computational graph:\n",
    "\n",
    "1. Forward pass: Compute activations\n",
    "2. Backward pass: Compute gradients\n",
    "\n",
    "For ReLU:\n",
    "$$\\frac{\\partial \\text{ReLU}}{\\partial x} = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{otherwise} \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "    \"\"\"Two-layer fully-connected neural network.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim: int = 3072, hidden_dim: int = 100, num_classes: int = 10):\n",
    "        \"\"\"Initialize network with Xavier/He initialization.\"\"\"\n",
    "        self.params = {}\n",
    "        self.params['W1'] = np.random.randn(input_dim, hidden_dim) * np.sqrt(2.0 / input_dim)\n",
    "        self.params['b1'] = np.zeros(hidden_dim)\n",
    "        self.params['W2'] = np.random.randn(hidden_dim, num_classes) * np.sqrt(2.0 / hidden_dim)\n",
    "        self.params['b2'] = np.zeros(num_classes)\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> Tuple[np.ndarray, Dict]:\n",
    "        \"\"\"Forward pass with caching for backprop.\n",
    "        \n",
    "        Returns:\n",
    "            scores: (N, C) class scores\n",
    "            cache: Dictionary with intermediate values\n",
    "        \"\"\"\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        \n",
    "        # Layer 1: Linear + ReLU\n",
    "        z1 = X @ W1 + b1  # (N, H)\n",
    "        h1 = np.maximum(0, z1)  # ReLU\n",
    "        \n",
    "        # Layer 2: Linear\n",
    "        scores = h1 @ W2 + b2  # (N, C)\n",
    "        \n",
    "        cache = {'X': X, 'z1': z1, 'h1': h1}\n",
    "        return scores, cache\n",
    "    \n",
    "    def loss(self, X: np.ndarray, y: np.ndarray, reg: float = 0.0) -> Tuple[float, Dict]:\n",
    "        \"\"\"Compute loss and gradients.\n",
    "        \n",
    "        Returns:\n",
    "            loss: Scalar loss\n",
    "            grads: Dictionary with gradients for each parameter\n",
    "        \"\"\"\n",
    "        N = X.shape[0]\n",
    "        \n",
    "        # Forward pass\n",
    "        scores, cache = self.forward(X)\n",
    "        \n",
    "        # Compute softmax loss\n",
    "        scores -= np.max(scores, axis=1, keepdims=True)  # Numerical stability\n",
    "        exp_scores = np.exp(scores)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        \n",
    "        loss = -np.sum(np.log(probs[range(N), y] + 1e-10)) / N\n",
    "        loss += reg * (np.sum(self.params['W1']**2) + np.sum(self.params['W2']**2))\n",
    "        \n",
    "        # Backward pass\n",
    "        grads = {}\n",
    "        \n",
    "        # Gradient on scores\n",
    "        dscores = probs.copy()\n",
    "        dscores[range(N), y] -= 1\n",
    "        dscores /= N\n",
    "        \n",
    "        # Layer 2 gradients\n",
    "        grads['W2'] = cache['h1'].T @ dscores + 2 * reg * self.params['W2']\n",
    "        grads['b2'] = np.sum(dscores, axis=0)\n",
    "        \n",
    "        # Backprop to hidden layer\n",
    "        dh1 = dscores @ self.params['W2'].T\n",
    "        \n",
    "        # ReLU backward\n",
    "        dz1 = dh1 * (cache['z1'] > 0)  # ReLU derivative\n",
    "        \n",
    "        # Layer 1 gradients\n",
    "        grads['W1'] = cache['X'].T @ dz1 + 2 * reg * self.params['W1']\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "        \n",
    "        return loss, grads\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        scores, _ = self.forward(X)\n",
    "        return np.argmax(scores, axis=1)\n",
    "    \n",
    "    def accuracy(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"Compute accuracy.\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "\n",
    "\n",
    "def train_neural_network(net: TwoLayerNet,\n",
    "                        X_train: np.ndarray,\n",
    "                        y_train: np.ndarray,\n",
    "                        X_val: np.ndarray,\n",
    "                        y_val: np.ndarray,\n",
    "                        learning_rate: float = 1e-3,\n",
    "                        reg: float = 1e-5,\n",
    "                        num_iters: int = 2000,\n",
    "                        batch_size: int = 200,\n",
    "                        verbose: bool = True) -> Dict:\n",
    "    \"\"\"Train neural network using SGD with momentum.\"\"\"\n",
    "    N = X_train.shape[0]\n",
    "    loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_acc_history = []\n",
    "    \n",
    "    # Use momentum\n",
    "    velocity = {key: np.zeros_like(val) for key, val in net.params.items()}\n",
    "    momentum = 0.9\n",
    "    \n",
    "    for it in range(num_iters):\n",
    "        # Sample mini-batch\n",
    "        batch_indices = np.random.choice(N, batch_size)\n",
    "        X_batch = X_train[batch_indices]\n",
    "        y_batch = y_train[batch_indices]\n",
    "        \n",
    "        # Compute loss and gradients\n",
    "        loss, grads = net.loss(X_batch, y_batch, reg)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # Update with momentum\n",
    "        for param_name in net.params:\n",
    "            velocity[param_name] = momentum * velocity[param_name] - learning_rate * grads[param_name]\n",
    "            net.params[param_name] += velocity[param_name]\n",
    "        \n",
    "        # Check accuracy periodically\n",
    "        if it % 200 == 0:\n",
    "            train_acc = net.accuracy(X_train[:1000], y_train[:1000])\n",
    "            val_acc = net.accuracy(X_val, y_val)\n",
    "            train_acc_history.append(train_acc)\n",
    "            val_acc_history.append(val_acc)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Iter {it:4d}: Loss={loss:.4f}, Train={train_acc:.2%}, Val={val_acc:.2%}\")\n",
    "    \n",
    "    return {\n",
    "        'loss_history': loss_history,\n",
    "        'train_acc_history': train_acc_history,\n",
    "        'val_acc_history': val_acc_history\n",
    "    }\n",
    "\n",
    "\n",
    "# Train neural network\n",
    "print(\"Training 2-Layer Neural Network...\\n\")\n",
    "\n",
    "net = TwoLayerNet(input_dim=3072, hidden_dim=100, num_classes=10)\n",
    "nn_history = train_neural_network(\n",
    "    net, X_train_flat, y_train, X_val_flat, y_val,\n",
    "    learning_rate=1e-3,\n",
    "    reg=1e-5,\n",
    "    num_iters=2000\n",
    ")\n",
    "\n",
    "# Visualize training\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot 1: Loss curve\n",
    "axes[0].plot(nn_history['loss_history'], linewidth=2, color='darkblue')\n",
    "axes[0].set_xlabel('Iteration', fontsize=11)\n",
    "axes[0].set_ylabel('Loss', fontsize=11)\n",
    "axes[0].set_title('Training Loss', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Accuracy curves\n",
    "iters_check = np.arange(0, 2000, 200)\n",
    "axes[1].plot(iters_check, nn_history['train_acc_history'], 'o-', label='Train', linewidth=2)\n",
    "axes[1].plot(iters_check, nn_history['val_acc_history'], 's-', label='Validation', linewidth=2)\n",
    "axes[1].set_xlabel('Iteration', fontsize=11)\n",
    "axes[1].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[1].set_title('Train vs Validation Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Visualize first layer weights\n",
    "W1 = net.params['W1'].T  # (H, D)\n",
    "W1_img = W1[:64].reshape(64, 32, 32, 3)  # First 64 neurons\n",
    "W1_grid = np.zeros((32*8, 32*8, 3))\n",
    "for i in range(64):\n",
    "    row, col = i // 8, i % 8\n",
    "    w = W1_img[i]\n",
    "    w_norm = (w - w.min()) / (w.max() - w.min() + 1e-10)\n",
    "    W1_grid[row*32:(row+1)*32, col*32:(col+1)*32] = w_norm\n",
    "\n",
    "axes[2].imshow(W1_grid)\n",
    "axes[2].set_title('First Layer Weights (Filters)', fontsize=12, fontweight='bold')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test accuracy\n",
    "test_acc_nn = net.accuracy(X_test_flat, y_test)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(\"Test Accuracy Comparison:\")\n",
    "print(f\"  Neural Network: {test_acc_nn:.2%}\")\n",
    "print(f\"  Softmax:        {test_acc_softmax:.2%}\")\n",
    "print(f\"  kNN:            {max(max(accuracies_l1), max(accuracies_l2)):.2%}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nðŸ”‘ Key insights:\")\n",
    "print(\"   â€¢ Nonlinearity (ReLU) enables learning complex functions\")\n",
    "print(\"   â€¢ Hidden layer learns features, output layer classifies\")\n",
    "print(\"   â€¢ Neural network >> linear classifier!\")\n",
    "print(\"   â€¢ First layer weights look like edge/color detectors\")\n",
    "print(\"   â€¢ More layers = more capacity (but also harder to train)\")\n",
    "print(\"\\nâœ“ Neural networks complete! Now let's add conv layers...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6: Convolutional Neural Networks (CNNs)\n",
    "\n",
    "**Key insight**: Images have spatial structure! Fully-connected layers ignore this.\n",
    "\n",
    "## Convolutional Layer\n",
    "\n",
    "Apply filters (kernels) to local regions:\n",
    "$$y[i,j] = \\sum_{m,n} W[m,n] \\cdot x[i+m, j+n]$$\n",
    "\n",
    "**Parameters**:\n",
    "- Filter size: $K \\times K$ (typically 3Ã—3 or 5Ã—5)\n",
    "- Stride: How much to move filter (typically 1 or 2)\n",
    "- Padding: Add zeros around border to preserve size\n",
    "\n",
    "**Output size**:\n",
    "$$H_{\\text{out}} = \\frac{H + 2P - K}{S} + 1$$\n",
    "\n",
    "where $P$ = padding, $S$ = stride.\n",
    "\n",
    "## Max Pooling\n",
    "\n",
    "Downsample by taking maximum in each region:\n",
    "$$y[i,j] = \\max_{m,n \\in \\text{region}} x[m,n]$$\n",
    "\n",
    "**Benefits**:\n",
    "- Reduces spatial size\n",
    "- Translation invariance\n",
    "- Controls overfitting\n",
    "\n",
    "## Why CNNs Work\n",
    "\n",
    "1. **Parameter sharing**: Same filter applied everywhere (much fewer params than FC)\n",
    "2. **Local connectivity**: Each neuron only looks at local patch\n",
    "3. **Translation invariance**: Same features everywhere in image\n",
    "4. **Hierarchical features**: Early layers = edges, late layers = objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_forward(X: np.ndarray, W: np.ndarray, b: np.ndarray, \n",
    "                   stride: int = 1, pad: int = 0) -> Tuple[np.ndarray, Dict]:\n",
    "    \"\"\"Forward pass for convolutional layer.\n",
    "    \n",
    "    Args:\n",
    "        X: (N, C_in, H, W) input\n",
    "        W: (C_out, C_in, K, K) filters\n",
    "        b: (C_out,) biases\n",
    "        stride: Stride\n",
    "        pad: Padding\n",
    "    \n",
    "    Returns:\n",
    "        out: (N, C_out, H_out, W_out) output\n",
    "        cache: Tuple for backprop\n",
    "    \"\"\"\n",
    "    N, C_in, H, W = X.shape\n",
    "    C_out, _, K, _ = W.shape\n",
    "    \n",
    "    # Add padding\n",
    "    X_pad = np.pad(X, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant')\n",
    "    \n",
    "    # Output dimensions\n",
    "    H_out = (H + 2*pad - K) // stride + 1\n",
    "    W_out = (W + 2*pad - K) // stride + 1\n",
    "    \n",
    "    # Initialize output\n",
    "    out = np.zeros((N, C_out, H_out, W_out))\n",
    "    \n",
    "    # Naive implementation (loop-based, slow but clear)\n",
    "    for i in range(H_out):\n",
    "        for j in range(W_out):\n",
    "            h_start = i * stride\n",
    "            h_end = h_start + K\n",
    "            w_start = j * stride\n",
    "            w_end = w_start + K\n",
    "            \n",
    "            # Extract patch\n",
    "            X_patch = X_pad[:, :, h_start:h_end, w_start:w_end]  # (N, C_in, K, K)\n",
    "            \n",
    "            # Convolve each filter\n",
    "            for c in range(C_out):\n",
    "                out[:, c, i, j] = np.sum(X_patch * W[c], axis=(1, 2, 3)) + b[c]\n",
    "    \n",
    "    cache = (X, W, b, stride, pad)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def maxpool2d_forward(X: np.ndarray, pool_size: int = 2, stride: int = 2) -> Tuple[np.ndarray, Dict]:\n",
    "    \"\"\"Forward pass for max pooling layer.\n",
    "    \n",
    "    Args:\n",
    "        X: (N, C, H, W) input\n",
    "        pool_size: Size of pooling window\n",
    "        stride: Stride\n",
    "    \n",
    "    Returns:\n",
    "        out: (N, C, H_out, W_out) output\n",
    "        cache: Tuple for backprop\n",
    "    \"\"\"\n",
    "    N, C, H, W = X.shape\n",
    "    \n",
    "    H_out = (H - pool_size) // stride + 1\n",
    "    W_out = (W - pool_size) // stride + 1\n",
    "    \n",
    "    out = np.zeros((N, C, H_out, W_out))\n",
    "    \n",
    "    for i in range(H_out):\n",
    "        for j in range(W_out):\n",
    "            h_start = i * stride\n",
    "            h_end = h_start + pool_size\n",
    "            w_start = j * stride\n",
    "            w_end = w_start + pool_size\n",
    "            \n",
    "            # Max over spatial window\n",
    "            X_patch = X[:, :, h_start:h_end, w_start:w_end]\n",
    "            out[:, :, i, j] = np.max(X_patch, axis=(2, 3))\n",
    "    \n",
    "    cache = (X, pool_size, stride)\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "def relu_forward(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Forward pass for ReLU.\"\"\"\n",
    "    out = np.maximum(0, X)\n",
    "    cache = X\n",
    "    return out, cache\n",
    "\n",
    "\n",
    "# Test CNN layers\n",
    "print(\"Testing CNN layers...\\n\")\n",
    "\n",
    "# Test convolutional layer\n",
    "X_test = X_train[:10].transpose(0, 3, 1, 2)  # (N, C, H, W)\n",
    "W_test = np.random.randn(16, 3, 5, 5) * 0.01  # 16 filters, 5Ã—5, 3 channels\n",
    "b_test = np.zeros(16)\n",
    "\n",
    "out_conv, _ = conv2d_forward(X_test, W_test, b_test, stride=1, pad=2)\n",
    "print(f\"Conv layer: Input {X_test.shape} â†’ Output {out_conv.shape}\")\n",
    "\n",
    "# Test max pooling\n",
    "out_pool, _ = maxpool2d_forward(out_conv, pool_size=2, stride=2)\n",
    "print(f\"Max pool:   Input {out_conv.shape} â†’ Output {out_pool.shape}\")\n",
    "\n",
    "# Test ReLU\n",
    "out_relu, _ = relu_forward(out_pool)\n",
    "print(f\"ReLU:       Input {out_pool.shape} â†’ Output {out_relu.shape}\")\n",
    "\n",
    "# Visualize learned filters (example)\n",
    "fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "for i in range(16):\n",
    "    ax = axes[i // 4, i % 4]\n",
    "    # Visualize filter (normalize each channel separately)\n",
    "    filt = W_test[i].transpose(1, 2, 0)  # (K, K, 3)\n",
    "    filt_norm = (filt - filt.min()) / (filt.max() - filt.min() + 1e-10)\n",
    "    ax.imshow(filt_norm)\n",
    "    ax.set_title(f'Filter {i}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.suptitle('Random Conv Filters (5Ã—5, 3 channels)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ”‘ Key insights:\")\n",
    "print(\"   â€¢ Conv layer: Apply filters to local regions\")\n",
    "print(\"   â€¢ Parameter sharing: Same filter everywhere (far fewer params)\")\n",
    "print(\"   â€¢ Max pooling: Downsample, translation invariance\")\n",
    "print(\"   â€¢ ReLU: Nonlinearity, fast and effective\")\n",
    "print(\"   â€¢ Stacking: Conv â†’ ReLU â†’ Pool â†’ repeat\")\n",
    "print(\"\\nâœ“ CNN layers complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7: Complete CNN Architecture - Mini AlexNet\n",
    "\n",
    "Let's build a simplified AlexNet for our 32Ã—32 images.\n",
    "\n",
    "## AlexNet Architecture (Simplified)\n",
    "\n",
    "```\n",
    "Input: 32Ã—32Ã—3\n",
    "â†“\n",
    "Conv1: 32 filters, 5Ã—5, stride 1, pad 2 â†’ 32Ã—32Ã—32\n",
    "ReLU â†’ MaxPool (2Ã—2, stride 2) â†’ 16Ã—16Ã—32\n",
    "â†“\n",
    "Conv2: 64 filters, 3Ã—3, stride 1, pad 1 â†’ 16Ã—16Ã—64\n",
    "ReLU â†’ MaxPool (2Ã—2, stride 2) â†’ 8Ã—8Ã—64\n",
    "â†“\n",
    "Flatten â†’ 4096\n",
    "â†“\n",
    "FC1: 4096 â†’ 256\n",
    "ReLU\n",
    "â†“\n",
    "FC2: 256 â†’ 10\n",
    "Softmax\n",
    "```\n",
    "\n",
    "## Parameter Count\n",
    "\n",
    "**Conv layers**: \n",
    "- Conv1: 32 Ã— (5Ã—5Ã—3 + 1) = 2,432\n",
    "- Conv2: 64 Ã— (3Ã—3Ã—32 + 1) = 18,496\n",
    "\n",
    "**FC layers**:\n",
    "- FC1: 4096 Ã— 256 = 1,048,576\n",
    "- FC2: 256 Ã— 10 = 2,560\n",
    "\n",
    "**Total**: ~1.07M parameters (vs 30M for pure FC!)\n",
    "\n",
    "**Insight**: CNNs are much more parameter-efficient than FC networks for images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN:\n",
    "    \"\"\"Simple CNN for image classification (toy AlexNet).\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize with He initialization.\"\"\"\n",
    "        self.params = {}\n",
    "        \n",
    "        # Conv1: 3 â†’ 32, 5Ã—5\n",
    "        self.params['W1'] = np.random.randn(32, 3, 5, 5) * np.sqrt(2.0 / (3*5*5))\n",
    "        self.params['b1'] = np.zeros(32)\n",
    "        \n",
    "        # Conv2: 32 â†’ 64, 3Ã—3\n",
    "        self.params['W2'] = np.random.randn(64, 32, 3, 3) * np.sqrt(2.0 / (32*3*3))\n",
    "        self.params['b2'] = np.zeros(64)\n",
    "        \n",
    "        # FC1: 4096 â†’ 256\n",
    "        self.params['W3'] = np.random.randn(4096, 256) * np.sqrt(2.0 / 4096)\n",
    "        self.params['b3'] = np.zeros(256)\n",
    "        \n",
    "        # FC2: 256 â†’ 10\n",
    "        self.params['W4'] = np.random.randn(256, 10) * np.sqrt(2.0 / 256)\n",
    "        self.params['b4'] = np.zeros(10)\n",
    "    \n",
    "    def forward(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass (inference mode, simplified).\n",
    "        \n",
    "        Args:\n",
    "            X: (N, H, W, C) input images\n",
    "        \n",
    "        Returns:\n",
    "            scores: (N, 10) class scores\n",
    "        \"\"\"\n",
    "        # Convert to (N, C, H, W) for conv layers\n",
    "        X = X.transpose(0, 3, 1, 2)\n",
    "        \n",
    "        # Conv1 â†’ ReLU â†’ Pool\n",
    "        out, _ = conv2d_forward(X, self.params['W1'], self.params['b1'], stride=1, pad=2)\n",
    "        out, _ = relu_forward(out)\n",
    "        out, _ = maxpool2d_forward(out, pool_size=2, stride=2)\n",
    "        \n",
    "        # Conv2 â†’ ReLU â†’ Pool\n",
    "        out, _ = conv2d_forward(out, self.params['W2'], self.params['b2'], stride=1, pad=1)\n",
    "        out, _ = relu_forward(out)\n",
    "        out, _ = maxpool2d_forward(out, pool_size=2, stride=2)\n",
    "        \n",
    "        # Flatten\n",
    "        N = out.shape[0]\n",
    "        out = out.reshape(N, -1)  # (N, 4096)\n",
    "        \n",
    "        # FC1 â†’ ReLU\n",
    "        out = out @ self.params['W3'] + self.params['b3']\n",
    "        out = np.maximum(0, out)\n",
    "        \n",
    "        # FC2\n",
    "        scores = out @ self.params['W4'] + self.params['b4']\n",
    "        \n",
    "        return scores\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        scores = self.forward(X)\n",
    "        return np.argmax(scores, axis=1)\n",
    "    \n",
    "    def accuracy(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"Compute accuracy.\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "\n",
    "\n",
    "# Test CNN (note: full training would be slow in pure NumPy, so we'll test architecture)\n",
    "print(\"Building SimpleCNN (toy AlexNet)...\\n\")\n",
    "\n",
    "cnn = SimpleCNN()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.size for p in cnn.params.values())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "X_sample = X_train[:5]\n",
    "scores = cnn.forward(X_sample)\n",
    "print(f\"\\nForward pass test:\")\n",
    "print(f\"  Input shape:  {X_sample.shape}\")\n",
    "print(f\"  Output shape: {scores.shape}\")\n",
    "print(f\"  Predictions:  {cnn.predict(X_sample)}\")\n",
    "\n",
    "# Random initialization accuracy\n",
    "random_acc = cnn.accuracy(X_val[:100], y_val[:100])\n",
    "print(f\"\\nRandom initialization accuracy: {random_acc:.2%} (expected ~10% for 10 classes)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CNN Architecture Summary\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Layer 1: Conv (3â†’32, 5Ã—5) + ReLU + MaxPool  â†’  16Ã—16Ã—32\")\n",
    "print(f\"Layer 2: Conv (32â†’64, 3Ã—3) + ReLU + MaxPool â†’  8Ã—8Ã—64\")\n",
    "print(f\"Layer 3: Flatten                             â†’  4096\")\n",
    "print(f\"Layer 4: FC (4096â†’256) + ReLU                â†’  256\")\n",
    "print(f\"Layer 5: FC (256â†’10)                         â†’  10\")\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Equivalent FC network: ~30,000,000 parameters (30Ã— more!)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸ”‘ Key insights:\")\n",
    "print(\"   â€¢ CNNs: Stack Conv+ReLU+Pool, then FC layers\")\n",
    "print(\"   â€¢ Parameter efficiency: 1M params vs 30M for FC\")\n",
    "print(\"   â€¢ Spatial hierarchy: Early = edges, Late = objects\")\n",
    "print(\"   â€¢ AlexNet (2012): First ImageNet breakthrough with CNNs\")\n",
    "print(\"   â€¢ Modern CNNs: ResNet, EfficientNet, etc. (same principles!)\")\n",
    "print(\"\\nâœ“ CNN architecture complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 8: Visualization, Saliency Maps, and Transfer Learning\n",
    "\n",
    "## Visualization Techniques\n",
    "\n",
    "### 1. Filter Visualization\n",
    "- Show what first-layer filters look like\n",
    "- Early layers: edges, colors, textures\n",
    "\n",
    "### 2. Activation Maps\n",
    "- Show which neurons activate for given input\n",
    "- See what features network detects\n",
    "\n",
    "### 3. Saliency Maps\n",
    "- Compute gradient of output w.r.t. input: $\\frac{\\partial y_c}{\\partial x}$\n",
    "- Shows which pixels matter most for prediction\n",
    "\n",
    "### 4. Class Visualization\n",
    "- Generate image that maximizes class score\n",
    "- Reveals what network thinks each class looks like\n",
    "\n",
    "## Transfer Learning\n",
    "\n",
    "**Key insight**: Features from ImageNet transfer to other tasks!\n",
    "\n",
    "**Strategy**:\n",
    "1. Pre-train on large dataset (ImageNet)\n",
    "2. Replace final layer for new task\n",
    "3. Fine-tune on small dataset\n",
    "\n",
    "**Why it works**: Early layers learn universal features (edges, textures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_saliency_map(net: SimpleCNN, X: np.ndarray, y: int) -> np.ndarray:\n",
    "    \"\"\"Compute saliency map for a single image.\n",
    "    \n",
    "    Args:\n",
    "        net: Trained network\n",
    "        X: (H, W, C) single image\n",
    "        y: Target class\n",
    "    \n",
    "    Returns:\n",
    "        saliency: (H, W) saliency map\n",
    "    \"\"\"\n",
    "    X = X[np.newaxis, ...]  # Add batch dimension\n",
    "    \n",
    "    # Forward pass\n",
    "    scores = net.forward(X)\n",
    "    \n",
    "    # Approximate gradient using finite differences\n",
    "    # (Full backprop implementation omitted for brevity)\n",
    "    eps = 1e-5\n",
    "    saliency = np.zeros((32, 32))\n",
    "    \n",
    "    # Sample-based approximation (for speed)\n",
    "    for i in range(0, 32, 4):\n",
    "        for j in range(0, 32, 4):\n",
    "            # Perturb pixel\n",
    "            X_perturb = X.copy()\n",
    "            X_perturb[0, i, j, :] += eps\n",
    "            \n",
    "            # Compute score change\n",
    "            scores_perturb = net.forward(X_perturb)\n",
    "            grad_approx = (scores_perturb[0, y] - scores[0, y]) / eps\n",
    "            saliency[i:i+4, j:j+4] = abs(grad_approx)\n",
    "    \n",
    "    return saliency\n",
    "\n",
    "\n",
    "def visualize_filters_and_activations(cnn: SimpleCNN, X_sample: np.ndarray):\n",
    "    \"\"\"Visualize learned filters and activation maps.\"\"\"\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(15, 12))\n",
    "    \n",
    "    # Row 1: Input images\n",
    "    for i in range(4):\n",
    "        axes[0, i].imshow(X_sample[i])\n",
    "        axes[0, i].set_title(f'Input {i}')\n",
    "        axes[0, i].axis('off')\n",
    "    \n",
    "    # Row 2: First layer filters (sample)\n",
    "    W1 = cnn.params['W1']  # (32, 3, 5, 5)\n",
    "    for i in range(4):\n",
    "        filt = W1[i].transpose(1, 2, 0)  # (5, 5, 3)\n",
    "        filt_norm = (filt - filt.min()) / (filt.max() - filt.min() + 1e-10)\n",
    "        axes[1, i].imshow(filt_norm)\n",
    "        axes[1, i].set_title(f'Filter {i}')\n",
    "        axes[1, i].axis('off')\n",
    "    \n",
    "    # Row 3: Saliency maps\n",
    "    for i in range(4):\n",
    "        y_pred = cnn.predict(X_sample[i:i+1])[0]\n",
    "        saliency = compute_saliency_map(cnn, X_sample[i], y_pred)\n",
    "        axes[2, i].imshow(saliency, cmap='hot')\n",
    "        axes[2, i].set_title(f'Saliency (pred={y_pred})')\n",
    "        axes[2, i].axis('off')\n",
    "    \n",
    "    plt.suptitle('CNN Visualization: Filters, Activations, Saliency', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize CNN\n",
    "print(\"Visualizing CNN components...\\n\")\n",
    "\n",
    "visualize_filters_and_activations(cnn, X_val[:4])\n",
    "\n",
    "print(\"\\nðŸ”‘ Key insights from visualization:\")\n",
    "print(\"   â€¢ First layer filters: Learn edge/color/texture detectors\")\n",
    "print(\"   â€¢ Saliency maps: Show which pixels matter for prediction\")\n",
    "print(\"   â€¢ Activation maps: Reveal what features network detects\")\n",
    "print(\"   â€¢ Class visualization: Generate prototypical examples\")\n",
    "\n",
    "print(\"\\nðŸŽ“ Transfer Learning Strategy:\")\n",
    "print(\"   1. Pre-train on ImageNet (millions of images)\")\n",
    "print(\"   2. Keep conv layers (feature extractor)\")\n",
    "print(\"   3. Replace FC layers for new task\")\n",
    "print(\"   4. Fine-tune on small target dataset\")\n",
    "print(\"   â†’ Works because early features are universal!\")\n",
    "\n",
    "print(\"\\nâœ“ Visualization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 9: Babysitting the Learning Process - Practical Tips\n",
    "\n",
    "## CS231n's Wisdom for Training Neural Networks\n",
    "\n",
    "### 1. Data Preprocessing\n",
    "- **Normalize**: Mean 0, std 1\n",
    "- **Augmentation**: Flips, crops, color jitter\n",
    "- **Whitening**: Decorrelate features (PCA)\n",
    "\n",
    "### 2. Weight Initialization\n",
    "- **Xavier**: $W \\sim N(0, 1/\\sqrt{n_{\\text{in}}})$ for tanh\n",
    "- **He**: $W \\sim N(0, 2/\\sqrt{n_{\\text{in}}})$ for ReLU\n",
    "- **Biases**: Usually 0\n",
    "\n",
    "### 3. Sanity Checks\n",
    "- **Overfit tiny dataset**: Should get ~100% accuracy\n",
    "- **Check loss**: Initial loss should match theory\n",
    "  - Softmax with C classes: $-\\log(1/C)$\n",
    "- **Gradient check**: Numerical vs analytical gradients\n",
    "\n",
    "### 4. Hyperparameter Tuning\n",
    "- **Learning rate**: Most important!\n",
    "  - Too high: Loss explodes\n",
    "  - Too low: No learning\n",
    "  - Sweet spot: Loss decreases steadily\n",
    "- **Regularization**: Start with 1e-5, tune on validation\n",
    "- **Batch size**: 32-256 typically\n",
    "\n",
    "### 5. Monitoring Training\n",
    "- **Loss curves**: Should decrease smoothly\n",
    "- **Train/val gap**: Indicates overfitting\n",
    "- **Weight updates**: ~1e-3 of weights per iteration\n",
    "- **Activation histograms**: Check for dead neurons\n",
    "\n",
    "### 6. Common Mistakes\n",
    "- Forgot to normalize data\n",
    "- Learning rate too high/low\n",
    "- Regularization too strong\n",
    "- Batch size too small (noisy gradients)\n",
    "- Not using validation set properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate babysitting tips\n",
    "\n",
    "def sanity_check_loss(num_classes: int = 10) -> float:\n",
    "    \"\"\"Expected initial loss for softmax with random weights.\"\"\"\n",
    "    return -np.log(1.0 / num_classes)\n",
    "\n",
    "\n",
    "def overfit_small_dataset(net: TwoLayerNet, X_small: np.ndarray, y_small: np.ndarray, num_iters: int = 500):\n",
    "    \"\"\"Sanity check: Should be able to overfit small dataset.\"\"\"\n",
    "    print(\"Sanity check: Overfitting 50 samples...\")\n",
    "    \n",
    "    losses = []\n",
    "    accs = []\n",
    "    \n",
    "    for it in range(num_iters):\n",
    "        loss, grads = net.loss(X_small, y_small, reg=0)  # No regularization\n",
    "        losses.append(loss)\n",
    "        accs.append(net.accuracy(X_small, y_small))\n",
    "        \n",
    "        # Large learning rate for overfitting\n",
    "        for param in net.params:\n",
    "            net.params[param] -= 1e-2 * grads[param]\n",
    "    \n",
    "    return losses, accs\n",
    "\n",
    "\n",
    "def plot_training_diagnostics(history: Dict):\n",
    "    \"\"\"Plot comprehensive training diagnostics.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    # Plot 1: Loss curve (log scale)\n",
    "    axes[0, 0].plot(history['loss_history'])\n",
    "    axes[0, 0].set_xlabel('Iteration')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Training Loss (log scale)')\n",
    "    axes[0, 0].set_yscale('log')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Train vs Val accuracy\n",
    "    iters = np.arange(0, len(history['loss_history']), len(history['loss_history'])//len(history['train_acc_history']))\n",
    "    axes[0, 1].plot(iters, history['train_acc_history'], label='Train')\n",
    "    axes[0, 1].plot(iters, history['val_acc_history'], label='Val')\n",
    "    axes[0, 1].set_xlabel('Iteration')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].set_title('Train/Val Accuracy Gap')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Learning rate schedule\n",
    "    iters_range = np.arange(len(history['loss_history']))\n",
    "    lrs = [learning_rate_schedule(1e-3, it, 'step') for it in iters_range]\n",
    "    axes[0, 2].plot(iters_range, lrs)\n",
    "    axes[0, 2].set_xlabel('Iteration')\n",
    "    axes[0, 2].set_ylabel('Learning Rate')\n",
    "    axes[0, 2].set_title('Learning Rate Schedule')\n",
    "    axes[0, 2].set_yscale('log')\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Loss histogram\n",
    "    axes[1, 0].hist(history['loss_history'][100:], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Loss')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].set_title('Loss Distribution')\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 5: Loss smoothness (gradient of loss)\n",
    "    loss_grad = np.diff(history['loss_history'])\n",
    "    axes[1, 1].plot(loss_grad, alpha=0.5)\n",
    "    axes[1, 1].plot(np.convolve(loss_grad, np.ones(50)/50, mode='valid'), linewidth=2, label='Smoothed')\n",
    "    axes[1, 1].set_xlabel('Iteration')\n",
    "    axes[1, 1].set_ylabel('Loss Gradient')\n",
    "    axes[1, 1].set_title('Loss Change Rate')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 6: Overfitting indicator\n",
    "    train_val_gap = np.array(history['train_acc_history']) - np.array(history['val_acc_history'])\n",
    "    axes[1, 2].plot(iters, train_val_gap, linewidth=2, color='red')\n",
    "    axes[1, 2].axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "    axes[1, 2].set_xlabel('Iteration')\n",
    "    axes[1, 2].set_ylabel('Train - Val Accuracy')\n",
    "    axes[1, 2].set_title('Overfitting Indicator')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    axes[1, 2].fill_between(iters, 0, train_val_gap, where=(train_val_gap > 0), \n",
    "                            color='red', alpha=0.3, label='Overfitting')\n",
    "    axes[1, 2].legend()\n",
    "    \n",
    "    plt.suptitle('Training Diagnostics: Babysitting the Learning Process', \n",
    "                fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run sanity checks\n",
    "print(\"=\" * 70)\n",
    "print(\"Babysitting Tips: Practical Training Checks\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check 1: Expected initial loss\n",
    "expected_loss = sanity_check_loss(10)\n",
    "print(f\"\\n1. Expected initial loss (10 classes): {expected_loss:.4f}\")\n",
    "print(f\"   (Random softmax: -log(1/10) = -log(0.1) â‰ˆ 2.303)\")\n",
    "\n",
    "# Check 2: Overfit small dataset\n",
    "print(\"\\n2. Sanity check: Overfitting 50 samples...\")\n",
    "small_net = TwoLayerNet()\n",
    "X_small = X_train_flat[:50]\n",
    "y_small = y_train[:50]\n",
    "\n",
    "losses_overfit, accs_overfit = overfit_small_dataset(small_net, X_small, y_small)\n",
    "print(f\"   Initial accuracy: {accs_overfit[0]:.2%}\")\n",
    "print(f\"   Final accuracy:   {accs_overfit[-1]:.2%}\")\n",
    "print(f\"   âœ“ Can overfit! (Should reach ~100%)\")\n",
    "\n",
    "# Check 3: Plot diagnostics\n",
    "print(\"\\n3. Training diagnostics (using previous neural network training)...\")\n",
    "plot_training_diagnostics(nn_history)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CS231n Babysitting Checklist:\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nâœ“ 1. Data preprocessing: Normalize, augment\")\n",
    "print(\"âœ“ 2. Weight initialization: Xavier/He\")\n",
    "print(\"âœ“ 3. Sanity checks: Overfit small set, check initial loss\")\n",
    "print(\"âœ“ 4. Learning rate: Start with 1e-3, tune carefully\")\n",
    "print(\"âœ“ 5. Monitor: Loss curves, train/val gap, gradients\")\n",
    "print(\"âœ“ 6. Regularization: Start weak, increase if overfitting\")\n",
    "print(\"\\nðŸ’¡ Rule of thumb: If loss doesn't decrease, check learning rate!\")\n",
    "print(\"\\nâœ“ Babysitting complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 10: Modern Architectures and Beyond\n",
    "\n",
    "CS231n provides the foundation. Modern architectures build on these principles.\n",
    "\n",
    "## VGG (2014)\n",
    "- **Key idea**: Stack many small (3Ã—3) convs\n",
    "- Deeper networks > wider networks\n",
    "- Simple, uniform architecture\n",
    "\n",
    "## ResNet (2015) - See Paper #10!\n",
    "- **Key idea**: Skip connections\n",
    "- $F(x) = H(x) - x$ (learn residual)\n",
    "- Enables training 1000+ layer networks\n",
    "- Solves degradation problem\n",
    "\n",
    "## Modern Trends (2020s)\n",
    "\n",
    "### Vision Transformers (ViT)\n",
    "- Replace convs with self-attention\n",
    "- Treat image as sequence of patches\n",
    "- Scales better than CNNs\n",
    "\n",
    "### EfficientNet\n",
    "- Compound scaling: depth + width + resolution\n",
    "- Neural architecture search\n",
    "- SOTA with fewer params\n",
    "\n",
    "### Diffusion Models\n",
    "- Generative models (DALL-E, Stable Diffusion)\n",
    "- Still use conv backbones!\n",
    "\n",
    "## The Big Picture\n",
    "\n",
    "CS231n teaches **timeless principles**:\n",
    "1. **Representation learning**: Learn features, not hand-craft\n",
    "2. **Hierarchical features**: Low-level â†’ high-level\n",
    "3. **Inductive biases**: CNNs for images, RNNs for sequences\n",
    "4. **Optimization**: Gradients, backprop, SGD\n",
    "5. **Regularization**: Prevent overfitting\n",
    "\n",
    "These apply to **all** of deep learningâ€”not just vision!\n",
    "\n",
    "---\n",
    "\n",
    "## Sutskever 30 Connection\n",
    "\n",
    "CS231n ties together multiple papers:\n",
    "- **#7**: AlexNet (CNNs for ImageNet)\n",
    "- **#10**: ResNet (skip connections)\n",
    "- **#11**: Dilated Convolutions (receptive fields)\n",
    "- **#13**: Transformers (attention for vision)\n",
    "\n",
    "**This notebook is your vision foundation!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary and comparison\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"CS231n: Complete Computer Vision Pipeline\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Summary table\n",
    "results_summary = {\n",
    "    'Method': ['kNN', 'Linear (Softmax)', 'Neural Network (2-layer)', 'CNN (Mini-AlexNet)'],\n",
    "    'Parameters': ['0 (memorize)', '~31K', '~1M', '~1M'],\n",
    "    'Accuracy': [f\"{max(max(accuracies_l1), max(accuracies_l2)):.1%}\", \n",
    "                f\"{test_acc_softmax:.1%}\",\n",
    "                f\"{test_acc_nn:.1%}\",\n",
    "                \"~60-70% (if trained)\"],\n",
    "    'Speed': ['Slow (test)', 'Fast', 'Fast', 'Medium'],\n",
    "    'Key Insight': ['No training', 'One template per class', 'Nonlinear features', 'Spatial structure']\n",
    "}\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"-\"*70)\n",
    "for i in range(len(results_summary['Method'])):\n",
    "    print(f\"{results_summary['Method'][i]:25s} | \"\n",
    "          f\"Params: {results_summary['Parameters'][i]:10s} | \"\n",
    "          f\"Acc: {results_summary['Accuracy'][i]:10s}\")\n",
    "    print(f\"{'':27s}   {results_summary['Key Insight'][i]}\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Key Takeaways from CS231n\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "takeaways = [\n",
    "    \"1. IMAGE CLASSIFICATION PIPELINE\",\n",
    "    \"   â€¢ Data â†’ Model â†’ Loss â†’ Optimization â†’ Prediction\",\n",
    "    \"   â€¢ Each component matters!\",\n",
    "    \"\",\n",
    "    \"2. MODEL EVOLUTION\",\n",
    "    \"   â€¢ kNN â†’ Linear â†’ NN â†’ CNN â†’ ResNet â†’ Transformers\",\n",
    "    \"   â€¢ Each step adds capacity and inductive bias\",\n",
    "    \"\",\n",
    "    \"3. CONVOLUTIONAL NETWORKS\",\n",
    "    \"   â€¢ Conv layers: Local connectivity, parameter sharing\",\n",
    "    \"   â€¢ Pooling: Downsampling, invariance\",\n",
    "    \"   â€¢ Hierarchy: Edges â†’ textures â†’ parts â†’ objects\",\n",
    "    \"\",\n",
    "    \"4. TRAINING TECHNIQUES\",\n",
    "    \"   â€¢ SGD with momentum, learning rate schedules\",\n",
    "    \"   â€¢ Xavier/He initialization\",\n",
    "    \"   â€¢ Regularization: L2, dropout, data augmentation\",\n",
    "    \"\",\n",
    "    \"5. BABYSITTING NEURAL NETS\",\n",
    "    \"   â€¢ Sanity checks: overfit small set, check initial loss\",\n",
    "    \"   â€¢ Monitor: loss curves, train/val gap, gradients\",\n",
    "    \"   â€¢ Hyperparameter tuning: learning rate is most important!\",\n",
    "    \"\",\n",
    "    \"6. VISUALIZATION\",\n",
    "    \"   â€¢ Understand what network learns\",\n",
    "    \"   â€¢ Filters, activations, saliency maps\",\n",
    "    \"   â€¢ Debugging tool and interpretability\",\n",
    "    \"\",\n",
    "    \"7. TRANSFER LEARNING\",\n",
    "    \"   â€¢ Pre-train on ImageNet, fine-tune on target task\",\n",
    "    \"   â€¢ Early features are universal\",\n",
    "    \"   â€¢ Enables learning from small datasets\",\n",
    "]\n",
    "\n",
    "for line in takeaways:\n",
    "    print(line)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Beyond CS231n: Modern Vision\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nâ€¢ ResNet (2015): Skip connections â†’ 1000+ layers\")\n",
    "print(\"â€¢ DenseNet (2016): Dense connections\")\n",
    "print(\"â€¢ EfficientNet (2019): NAS + compound scaling\")\n",
    "print(\"â€¢ Vision Transformers (2020): Attention for vision\")\n",
    "print(\"â€¢ ConvNeXt (2022): Modernized CNNs\")\n",
    "print(\"â€¢ Diffusion Models (2022): DALL-E, Stable Diffusion\")\n",
    "print(\"\\nâ†’ All build on CS231n foundations!\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸŽ“ CS231n: Complete! You've learned vision from first principles.\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nWhat you can do now:\")\n",
    "print(\"  âœ“ Understand how CNNs work (from scratch!)\")\n",
    "print(\"  âœ“ Train vision models (optimization, regularization)\")\n",
    "print(\"  âœ“ Debug neural networks (babysitting tips)\")\n",
    "print(\"  âœ“ Read modern papers (you have the foundation!)\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  â†’ Implement in PyTorch for real datasets\")\n",
    "print(\"  â†’ Study ResNet (Paper #10 in this repo!)\")\n",
    "print(\"  â†’ Explore transformers (Paper #13)\")\n",
    "print(\"  â†’ Build your own vision systems!\")\n",
    "print(\"\\nâœ¨ Welcome to computer vision! âœ¨\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
